{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Keyword cleansing from Web of Science search result\n",
    "* search result = `keywords` + `abstract`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "### 1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re, nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import itertools\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "Lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. filename settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus = pd.read_csv(\"df_scopus.csv\")\n",
    "keyword_org_name = \"keywords.txt\"\n",
    "keyword_abb_name = \"keywords_abb.json\"\n",
    "keyword_dash_name = \"keywords_dash.json\"\n",
    "keyword_dict_name = \"keywords_dict.json\"\n",
    "keyword_single_name = \"keywords_single.txt\"\n",
    "keyword_plural_name = \"keywords_plural.json\"\n",
    "unicode_name = \"unicode_json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Search in Scopus\n",
    "* receive `scopus.csv` via email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data Retrieval from scopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybliometrics.scopus import AbstractRetrieval\n",
    "scopus = pd.read_csv(\"df_scopus.csv\")\n",
    "\n",
    "# e1fc56d1d9fcc86e6998a7fd79faed23\n",
    "# dcbed1c3abb36a8e80addbf185f38671\n",
    "# 78d8e510bda00c5530f560f02bc051bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, calendar\n",
    "\n",
    "mode = \"a\" if os.path.exists(filename) else \"w\"\n",
    "\n",
    "if mode == \"w\":\n",
    "    with open(filename, mode) as datafile:\n",
    "        datafile.write(\"FN Clarivate Analytics Web of Science\\nVR 1.0\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./BIPV_ML_all.txt\"\n",
    "# artno = 0\n",
    "\n",
    "### document type\n",
    "# \"J\" = Journal\n",
    "# \"B\" = Book\n",
    "# \"S\" = Series\n",
    "# \"P\" = Patent\n",
    "dic_docu_type = {'Article': \"J\", \n",
    "                 'Review': \"J\", \n",
    "                 'Conference Paper': \"J\",\n",
    "                 'Conference Review': \"J\",\n",
    "                 'Book Chapter': \"B\",\n",
    "                 'Erratum': \"J\", \n",
    "                 'Letter': \"J\",\n",
    "                 '[No source information available]': \"J\", \n",
    "                 'Note': \"J\", \n",
    "                 'Short Survey': \"J\",\n",
    "                 'Book': \"B\", \n",
    "                 'Retracted': \"B\", \n",
    "                 'Editorial': \"S\"}\n",
    "\n",
    "### language\n",
    "dic_language = {'eng': \"English\",\n",
    "                'kor': \"Korean\"\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "Scopus401Error",
     "evalue": "The requestor is not authorized to access the requested view or fields of the resource",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopus401Error\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-397068ad05e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0martno\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscopus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractRetrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscopus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0martno\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"FULL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscopus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Authors\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0martno\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'[No author name available]'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pybliometrics/scopus/abstract_retrieval.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, identifier, refresh, view, id_type)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m# Load json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         Retrieval.__init__(self, identifier=identifier, id_type=id_type,\n\u001b[0;32m--> 611\u001b[0;31m                            api='AbstractRetrieval', refresh=refresh, view=view)\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abstracts-retrieval-response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchained_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"item\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bibrecord\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"head\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pybliometrics/scopus/superclasses/retrieval.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, identifier, api, refresh, view, id_type, date)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Parse file contents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mqfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# print(self._json)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pybliometrics/scopus/superclasses/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fname, refresh, params, url, download, max_entries, verbose, *args, **kwds)\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msearch_request\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pybliometrics/scopus/utils/get_content.py\u001b[0m in \u001b[0;36mget_content\u001b[0;34m(url, params, *args, **kwds)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mScopus401Error\u001b[0m: The requestor is not authorized to access the requested view or fields of the resource"
     ]
    }
   ],
   "source": [
    "# this cell dies when the number of retieved paper > 10000/week\n",
    "\n",
    "for artno in range(scopus.shape[0]):\n",
    "    ab = AbstractRetrieval(scopus[\"EID\"].iloc[artno], view=\"FULL\")\n",
    "    \n",
    "    if scopus[\"Authors\"].iloc[artno] != '[No author name available]':   \n",
    "        mode = \"a\" if os.path.exists(filename) else \"w\"\n",
    "        with open(filename, mode) as datafile:\n",
    "            # 1. PT: publication type\n",
    "            docu_type_ = scopus[\"Document Type\"].iloc[artno]\n",
    "            try:\n",
    "                docu_type_0 = dic_docu_type[docu_type_]\n",
    "            except:\n",
    "                docu_type_0 = \"unknown\"\n",
    "            datafile.write(f\"PT {docu_type_0}\\n\")\n",
    "\n",
    "            # 2. AU: author names\n",
    "            indexed_name_ = pd.DataFrame(ab.authors)['indexed_name'].tolist()    \n",
    "            datafile.write(f\"AU {indexed_name_[0]}\\n\")\n",
    "            for i in range(1, len(indexed_name_)):\n",
    "                datafile.write(f\"   {indexed_name_[i]}\\n\")\n",
    "\n",
    "            # 3. AF: author names, full\n",
    "            full_name_ = pd.DataFrame(ab.authors)[[\"surname\", \"given_name\"]].apply(lambda s: f\"{s[0]}, {s[1]}\", axis=1).tolist()\n",
    "            datafile.write(f\"AF {full_name_[0]}\\n\")\n",
    "            for i in range(1, len(full_name_)):\n",
    "                datafile.write(f\"   {full_name_[i]}\\n\")\n",
    "\n",
    "            # 4. TI: document title\n",
    "            docu_title_ = scopus[\"Title\"].iloc[artno]\n",
    "            datafile.write(f\"TI {docu_title_}\\n\")\n",
    "\n",
    "            # 5. SO: publication name\n",
    "            src_title_ = scopus[\"Source title\"].iloc[artno]\n",
    "            datafile.write(f\"SO {src_title_}\\n\")\n",
    "\n",
    "            # 6. LA : Language\n",
    "            try:\n",
    "                language_ = dic_language[ab.language]\n",
    "            except:\n",
    "                language_ = \"unknown\"\n",
    "            datafile.write(f\"LA {language_}\\n\")\n",
    "\n",
    "            # 7. DT : Document Type\n",
    "            docu_type_ = scopus[\"Document Type\"].iloc[artno]\n",
    "            datafile.write(f\"DT {docu_type_}\\n\")\n",
    "\n",
    "            # 8. DE : Author Keywords\n",
    "            auth_kw_ = ab.authkeywords\n",
    "            if auth_kw_ == None:\n",
    "                datafile.write(\"DE None\\n\")\n",
    "            else:\n",
    "                datafile.write(f\"DE {'; '.join(auth_kw_)}\\n\")\n",
    "\n",
    "            # 9. ID : Keyword Plus\n",
    "            datafile.write(\"ID None\\n\")\n",
    "\n",
    "            # 10. AB : Abstract\n",
    "            datafile.write(f\"AB {ab.abstract}\\n\")\n",
    "\n",
    "            # 11. C1 : Author Address\n",
    "            tmp = pd.DataFrame(ab.authorgroup)\n",
    "            grouped = tmp.groupby('organization')\n",
    "            aff_ids = tmp[\"organization\"].unique()\n",
    "\n",
    "            if len(aff_ids) > 0:\n",
    "                for i, aff_id in enumerate(aff_ids):\n",
    "                    if aff_id != None:\n",
    "                        group = grouped.get_group(aff_id)\n",
    "                        names = group[[\"surname\", \"given_name\"]].apply(lambda s: f\"{s[0]}, {s[1]}\", axis=1).tolist()\n",
    "                        aff = group[[\"organization\", \"city\", \"postalcode\", \"addresspart\", \"country\"]].apply(lambda s: f\"{s[0]}, {s[1]}, {s[2]}, {s[3]}, {s[4]}\", axis=1).iloc[0]\n",
    "                        if i == 0:\n",
    "                            datafile.write('C1 [' + '; '.join(names) + f'] {aff}\\n')\n",
    "                        else:\n",
    "                            datafile.write('   [' + '; '.join(names) + f'] {aff}\\n')\n",
    "\n",
    "            # 12. RP : Reprint Address\n",
    "            datafile.write(f\"RP None\\n\")\n",
    "\n",
    "            # 13. EM : E-mail Address\n",
    "            datafile.write(f\"EM None\\n\")\n",
    "\n",
    "            # 14. CR : Cited References\n",
    "            if ab.references != None:\n",
    "                tmp = pd.DataFrame(ab.references)\n",
    "                refcount = int(ab.refcount)\n",
    "\n",
    "                for i in range(refcount):\n",
    "                    tmp_ = tmp.iloc[i]\n",
    "                    tmp_authors = tmp_['authors']\n",
    "                    if tmp_authors == None:\n",
    "                        tmp_authors = \"[Anonymous]\"\n",
    "                    tmp_year = tmp_['publicationyear']\n",
    "                    tmp_src = tmp_['sourcetitle']\n",
    "                    tmp_vol = tmp_['volume']\n",
    "                    tmp_page = tmp_['first']\n",
    "                    tmp_doi = tmp_['doi']\n",
    "\n",
    "                    ref = tmp_authors\n",
    "                    for item in [tmp_year, tmp_src, tmp_vol, tmp_page]:\n",
    "                        if item != None:\n",
    "                            ref = ', '.join([ref, item])\n",
    "                    if tmp_doi != None:\n",
    "                        ref = ref + f\", DOI {tmp_doi}\"\n",
    "\n",
    "                    if i == 0:\n",
    "                        datafile.write(f\"CR {ref}\\n\")\n",
    "                    else:\n",
    "                        datafile.write(f\"   {ref}\\n\")\n",
    "\n",
    "            # 15. NR : Cited Reference Count\n",
    "            datafile.write(f\"NR {refcount}\\n\")\n",
    "\n",
    "            # 16. TC : Web of Science Core Collection Times Cited Count\n",
    "            citecount = ab.citedby_count\n",
    "            datafile.write(f\"TC {citecount}\\n\")\n",
    "\n",
    "            # 17. Z9 : Total Times Cited Count\n",
    "            datafile.write(f\"Z9 {citecount}\\n\")\n",
    "\n",
    "            # 18. U1 : Usage Count (Last 180 Days)\n",
    "            # 19. U2 : Usage Count (Since 2013)\n",
    "            # 20. PU : Publisher = ELSEVIER SCI LTD\n",
    "            # 21. PI : Publisher City = OXFORD\n",
    "            # 22. PA : Publisher Address = THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND\n",
    "            # 23. SN : International Standard Serial Number (ISSN) = 0959-6526\n",
    "            if ab.issn != None:\n",
    "                datafile.write(f\"SN {ab.issn}\\n\")\n",
    "\n",
    "            # 24. EI : Electronic International Standard Serial Number (eISSN) = 1879-1786\n",
    "            # 25. J9 : 29-Character Source Abbreviation = J CLEAN PROD\n",
    "            if ab.sourcetitle_abbreviation != None:\n",
    "                datafile.write(f\"J9 {ab.sourcetitle_abbreviation.upper()}\\n\")\n",
    "\n",
    "            # 26. JI : ISO Source Abbreviation = J. Clean Prod.\n",
    "                datafile.write(f\"JI {ab.sourcetitle_abbreviation}\\n\")\n",
    "\n",
    "            # 27. PD : Publication Date = JUL 1\n",
    "            if ab.coverDate != None:\n",
    "                month = ab.coverDate.split('-')[1]\n",
    "                date = ab.coverDate.split('-')[2]\n",
    "                datafile.write(f\"PD {calendar.month_name[int(month)][:3].upper()} {int(date)}\\n\")\n",
    "\n",
    "            # 28. PY : Publication Year = 2020\n",
    "            if scopus['Year'].iloc[artno] != None:\n",
    "                datafile.write(f\"PY {scopus['Year'].iloc[artno]}\\n\")\n",
    "\n",
    "            # 29. VL : Volumn = 260\n",
    "            if scopus['Volume'].iloc[artno] != None:\n",
    "                datafile.write(f\"VL {scopus['Volume'].iloc[artno]}\\n\")\n",
    "\n",
    "            # 30. AR : Article Number = 121059\n",
    "            if scopus['Art. No.'].iloc[artno] != None:\n",
    "                datafile.write(f\"AR {scopus['Art. No.'].iloc[artno]}\\n\")\n",
    "\n",
    "            # 31. DI : Digital Object Identifier = 10.1016/j.jclepro.2020.121059\n",
    "            if scopus['DOI'].iloc[artno] != None:\n",
    "                datafile.write(f\"DI {scopus['DOI'].iloc[artno]}\\n\")\n",
    "\n",
    "            # 32. PG : Page Count = 14\n",
    "            # 33. WC : Web of Science Categories = Green & Sustainable Science & Technology; Engineering, Environmental; Environmental Sciences\n",
    "            # 34. SC : Research Areas = Science & Technology - Other Topics; Engineering; Environmental Sciences & Ecology\n",
    "            if ab.subject_areas != None:\n",
    "                tmp = pd.DataFrame(ab.subject_areas)\n",
    "                tmp_ = tmp[\"area\"].tolist()\n",
    "                datafile.write(\"SC \" + \"; \".join(tmp_) + \"\\n\")\n",
    "            # 35. GA : Document Delivery Number = LL4XH\n",
    "            # 36. UT : Accession Number = WOS:000531559900003\n",
    "            # 37. DA : Date this report was generated. = 2020-06-14\n",
    "            datafile.write(f\"ER\\n\\n\")\n",
    "\n",
    "with open(filename, mode) as datafile:\n",
    "    datafile.write(f\"EF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Extract `keywords`\n",
    "* crawling WOS file and save as \"keywords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count= 1000\n",
      "count= 2000\n",
      "count= 3000\n",
      "count= 4000\n",
      "count= 5000\n",
      "count= 6000\n",
      "count= 7000\n",
      "count= 8000\n"
     ]
    }
   ],
   "source": [
    "scopus = pd.read_csv(\"scopus.csv\")\n",
    "\n",
    "keywords = []\n",
    "line = ''\n",
    "count = 0\n",
    "\n",
    "outfile = open(\"keywords.txt\", \"w\")\n",
    "outfile.close()\n",
    "\n",
    "with open(\"BIPV_ML_all.txt\") as f:\n",
    "    while line != \"EF\":\n",
    "        line = f.readline()\n",
    "        if line[:3] == \"DE \":\n",
    "            count += 1\n",
    "            if count%1000 == 0:\n",
    "                print(f\"count= {count}\")\n",
    "            keywords_new = line[3:].rstrip('\\n').split(\"; \")\n",
    "            \n",
    "            with open(\"keywords.txt\", \"a\") as outfile:\n",
    "                outfile.write(f'# {count}\\n')\n",
    "                for keyword_new in keywords_new:\n",
    "                    outfile.write(f'{keyword_new}\\n')\n",
    "            \n",
    "            keywords += keyword_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keyword Cleaninsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* store keywords in `keywords`\n",
    "* save `keywords` as `\"keywords.txt\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "line = ''\n",
    "\n",
    "with open(\"BIPV_ML_all.txt\") as f:\n",
    "    while line != \"EF\":\n",
    "        line = f.readline()\n",
    "        if line[:3] == \"DE \":\n",
    "            keywords_new = line[3:].rstrip('\\n').split(\";\")\n",
    "            for keyword_new in keywords_new:\n",
    "                keywords += [keyword_new.rstrip(' ').lstrip(' ')]\n",
    "\n",
    "keywords = sorted(keywords)\n",
    "with open(keyword_org_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords:\n",
    "        f.write(f\"{keyword}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Precedure\n",
    "\n",
    "\n",
    "1. get unique keywords\n",
    "2. detect and extract abbreviations as word in pharenthesis, starting from upper character: store in `keywords_abb_key_cand`.\n",
    "3. delete \"dirties\" on keywords : \"-\" and \":\" on each side of them, and store in `keywords_value`\n",
    "4. create abbreviation dictionary: `keywords_abb`\n",
    "5. remove mathematics and unicodes from `keywords_value`\n",
    "6. remove needless blanks from `keywords_value`\n",
    "7. create single-word keywords, remove plural forms and save as `keywords_single.txt`\n",
    "8. save as files\n",
    "  - `keywords_abb`: \"keywords_abb.json\"\n",
    "  - `keywords_single`: \"keywords_single.txt\"\n",
    "  - `keywords_dict`: \"keywords_dict.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unicode dictionary\n",
    "with open('unicode_dict.json') as j:\n",
    "    unicode_dict = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aerosols\n",
      "aggregators\n",
      "agrivoltaics\n",
      "algorithms\n",
      "barriers\n",
      "bifurcations\n",
      "buildings\n",
      "characteristics\n",
      "choppers\n",
      "clouds\n",
      "consumers\n",
      "controllers\n",
      "converters\n",
      "correlations\n",
      "costs\n",
      "delays\n",
      "dielectrics\n",
      "diodes\n",
      "dislocations\n",
      "dynamics\n",
      "economics\n",
      "eigenvalues\n",
      "electrolyzers\n",
      "emissions\n",
      "ensembles\n",
      "experiments\n",
      "failures\n",
      "faults\n",
      "feed-in-tariffs\n",
      "forecasts\n",
      "greenhouses\n",
      "harmonics\n",
      "heterojunctions\n",
      "households\n",
      "hurricanes\n",
      "ibscs\n",
      "igbts\n",
      "imports\n",
      "inverters\n",
      "irradiances\n",
      "measurements\n",
      "metaheuristics\n",
      "micro-grids\n",
      "microgrids\n",
      "microinverters\n",
      "microturbines\n",
      "mini-grids\n",
      "modes\n",
      "models\n",
      "modules\n",
      "multi-junctions\n",
      "nano-grids\n",
      "nanofluids\n",
      "nanowires\n",
      "optoelectronics\n",
      "performances\n",
      "perovskites\n",
      "photodetectors\n",
      "photodiodes\n",
      "photovoltaics\n",
      "prosumers\n",
      "relays\n",
      "renewables\n",
      "resonances\n",
      "scenarios\n",
      "semiconductors\n",
      "sensors\n",
      "simulations\n",
      "simulators\n",
      "supercapacitors\n",
      "systems\n",
      "techno-economics\n",
      "technoeconomics\n",
      "thermodynamics\n",
      "thermophotovoltaics\n",
      "transients\n",
      "trends\n",
      "voltages\n",
      "waves\n",
      "wavelets\n"
     ]
    }
   ],
   "source": [
    "# retreive keywords\n",
    "keywords_raw_ = np.genfromtxt(\"keywords.txt\", dtype=\"str\", delimiter=\"\\n\", encoding='utf-8-sig') #utf-8-sig for \\ufeff removal\n",
    "\n",
    "# get unique ones\n",
    "keywords_raw = np.unique(keywords_raw_)\n",
    "\n",
    "# abbriviations detection (key candidates)\n",
    "keywords_abb = {}\n",
    "keywords_abb_key_cand = []\n",
    "for keyword_raw_ in keywords_raw_:\n",
    "    kwr = keyword_raw_.split()\n",
    "    for kw in kwr:\n",
    "        kw = kw.lstrip(\"(\").rstrip(\")\")\n",
    "        if (kw == kw.upper()) \\\n",
    "           and (kw.lstrip(\"(\").rstrip(\")\") not in keywords_abb_key_cand) \\\n",
    "           and (ord(kw[0]) >= ord('A') and ord(kw[0]) <= ord('Z')) \\\n",
    "           and (len(kw) > 1):\n",
    "            keywords_abb_key_cand.append(kw.lstrip(\"(\").rstrip(\")\").rstrip(\":\").rstrip(\",\"))\n",
    "\n",
    "# convert to lower cases\n",
    "# keywords_raw = [kw.lower() for kw in keywords_raw_]\n",
    "\n",
    "# duplicate for values, and convert to lower cases\n",
    "keywords_value = deepcopy(keywords_raw)\n",
    "\n",
    "# convert to lower cases\n",
    "keywords_value = [kw.lower() for kw in keywords_value]\n",
    "\n",
    "# remove staring characters\n",
    "keywords_value = [kw.lstrip('-') for kw in keywords_value]\n",
    "keywords_value = [kw.rstrip('-') for kw in keywords_value]\n",
    "keywords_value = [kw.lstrip(':') for kw in keywords_value]\n",
    "keywords_value = [kw.rstrip(':') for kw in keywords_value]\n",
    "\n",
    "# remove brakets \"()\"\n",
    "for kc in keywords_abb_key_cand:\n",
    "    kc_ = kc.lower()\n",
    "    for kw in keywords_value:\n",
    "        if kc_ in kw.replace('(', \"\").replace(')', \"\").split():\n",
    "            kc_value = re.sub(r'\\([^)]*\\)', \"\", kw).rstrip(' ').lstrip(' ')\n",
    "            if (kc_ not in kc_value) and \\\n",
    "               (kc_value[-1] != \")\") and \\\n",
    "               (len(kc_value.split()) > 1) and \\\n",
    "               ((kc not in list(keywords_abb.keys())) or (kc in list(keywords_abb.keys()) and len(kc_value) < len(keywords_abb[kc]) and len(kc_value.split()) > 1)):\n",
    "                kc_value = kc_value.replace(\"  \", \" \")\n",
    "                kc_value = ' '.join(kc_value.split(' ')[:-1] + [Lem.lemmatize(kc_value.split(' ')[-1])])\n",
    "                keywords_abb.update({kc:kc_value})\n",
    "\n",
    "keywords_value = [re.sub(r'\\([^)]*\\)', \"\", kw).rstrip(' ').lstrip(' ').lstrip(')').lstrip('(') for kw in keywords_value]\n",
    "\n",
    "# remove unicodes â€“\n",
    "unicode_keys = list(unicode_dict.keys())\n",
    "for ukey in unicode_keys:\n",
    "        keywords_value = [kw.replace(ukey, unicode_dict[ukey]) for kw in keywords_value]\n",
    "\n",
    "# remove mathematics\n",
    "keywords_value = [kw.replace(\"\\\\infty\", \"infinity\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"\\\\mathrm{\", \"\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"{\", \"\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"}\", \"\") for kw in keywords_value]\n",
    "\n",
    "# remove '\"'\n",
    "keywords_value = [kw.replace('\"', \"\") for kw in keywords_value]\n",
    "\n",
    "# reduce needless blanks.\n",
    "keywords_value = [re.sub(\"\\s+\", \" \", kw) for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\" -\", \"-\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"- \", \"-\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\" //\", \"//\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"// \", \"//\") for kw in keywords_value]\n",
    "\n",
    "# create single-word keyword\n",
    "keywords_single = [k for k in list(keywords_abb.keys())]\n",
    "for kws in keywords_value:\n",
    "    if len(kws.split(\" \")) == 1 and len(kw) <= 4:\n",
    "        keywords_single.append(kws)\n",
    "    for kw in kws:\n",
    "        if len(kw.split(\"-\")) == 1 and len(kw) <= 4 and len(kw) > 1:\n",
    "            keywords_single.append(kw)\n",
    "\n",
    "keywords_single = list(np.unique(sorted(keywords_single)))\n",
    "\n",
    "keywords_single_ = deepcopy(keywords_single)       \n",
    "for kw in keywords_single_:\n",
    "    if kw+\"s\" in keywords_single and len(kw) >= 4:\n",
    "        print(kw+\"s\")\n",
    "        keywords_single.remove(kw+\"s\")\n",
    "    if kw[-1]+\"ies\" in keywords_single and kw[-1] == \"y\" and len(kw >=4):\n",
    "        print(kw+\"s\")\n",
    "        keywords_single.remove(kw+\"s\")\n",
    "\n",
    "# create abbreviation dictionary\n",
    "abb_sorted = dict(sorted(keywords_abb.items()))\n",
    "with open(keyword_abb_name, \"w\") as j:\n",
    "    json.dump(abb_sorted, j, ensure_ascii=False, indent=2)\n",
    "    \n",
    "# create \"single\" keywords list\n",
    "with open(keyword_single_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords_single:\n",
    "        f.write(f\"{keyword}\\n\")\n",
    "\n",
    "# create \"value\"\n",
    "keywords_dict = dict(zip(keywords_raw, keywords_value))\n",
    "with open(keyword_dict_name, \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. [manual] refine `keywords_single`\n",
    "* remove plural words not to be kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. create single-plural words dictionary\n",
    "* convert plural words in `keywords_dict`, overwrite as `keywords_dict.json`\n",
    "* create plural-singular dictionary: `keywords_single.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve keywords_value\n",
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)\n",
    "keywords_value = list(keywords_dict.values())\n",
    "keywords_key = list(keywords_dict.keys())\n",
    "\n",
    "# retreive keywords_single\n",
    "keywords_single = np.genfromtxt(\"keywords_single_refine.txt\", dtype=\"str\", delimiter=\"\\n\", encoding='utf-8-sig') #utf-8-sig for \\ufeff removal\n",
    "keywords_single = [k.lower() for k in keywords_single]\n",
    "\n",
    "# Convert plural to singular\n",
    "Lem = WordNetLemmatizer()\n",
    "keywords_plural = {}\n",
    "keywords_value_ = []\n",
    "for kws in keywords_value:\n",
    "    if (kws not in keywords_single) and (len(kws.split(\" \")) > 1):\n",
    "        kws_0 = kws.split(\" \")\n",
    "        \n",
    "        for kws_1 in kws_0:\n",
    "            kws_2 = kws_1.split(\"-\")\n",
    "            for kw in kws_2:\n",
    "                if (kw not in keywords_single) and (kw != Lem.lemmatize(kw)):\n",
    "                    kws = kws.replace(kw, Lem.lemmatize(kw))\n",
    "                    keywords_plural.update({kw: Lem.lemmatize(kw)})\n",
    "        keywords_value_.append(kws)\n",
    "    else:\n",
    "        keywords_value_.append(kws)\n",
    "\n",
    "keywords_value = deepcopy(keywords_value_)\n",
    "keywords_value = [kw.lstrip(' ').rstrip(' ') for kw in keywords_value]\n",
    "\n",
    "    \n",
    "# create plural-singular dictionary\n",
    "plural_sorted = dict(sorted(keywords_plural.items()))\n",
    "with open(keyword_plural_name, \"w\") as j:\n",
    "    json.dump(plural_sorted, j, ensure_ascii=False, indent=2)\n",
    "    \n",
    "# create \"value\"\n",
    "keywords_dict = dict(zip(keywords_key, keywords_value))\n",
    "with open(keyword_dict_name, \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. create dash-words dictionary\n",
    "* extract words with dash('-') from keywords_dict, save as `keywords_dash.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve keywords_value\n",
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)\n",
    "keywords_value = list(keywords_dict.values())\n",
    "\n",
    "# create dash dictionary\n",
    "keywords_dash = []\n",
    "for kw in keywords_value:\n",
    "    kw_ = kw.split(\" \")\n",
    "    for kw__ in kw_:\n",
    "        if ('-' in kw__) or ('//' in kw__):\n",
    "            keywords_dash.append(kw__)\n",
    "    \n",
    "keywords_dash = np.unique(keywords_dash)\n",
    "\n",
    "# create \"dash\" keywords dictionary - manual refinement required\n",
    "keywords_dash_dict = dict(zip(keywords_dash, keywords_dash))\n",
    "with open(keyword_dash_name, \"w\") as j:\n",
    "    json.dump(keywords_dash_dict, j, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. [manual] keywords-abb refinement\n",
    "* incorrect names, typo errors\n",
    "* unifying same words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. [manual] keywords-dash refinement\n",
    "* incorrect names, typo errors\n",
    "* unifying same words (ex. photo-voltaic and photovoltaic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_plural.json\", \"r\") as j:\n",
    "    keywords_plural = json.load(j)\n",
    "\n",
    "with open(\"keywords_abb_refine.json\", \"r\") as j:\n",
    "    keywords_abb_refine = json.load(j)\n",
    "\n",
    "with open(\"keywords_dash_refine.json\", \"r\") as j:\n",
    "    keywords_dash_refine = json.load(j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "abb_refine_keys = list(keywords_abb_refine.keys())\n",
    "dash_refine_keys = list(keywords_dash_refine.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dash_refine2 = {}\n",
    "\n",
    "plural_keys = list(keywords_plural.keys())\n",
    "\n",
    "for dkey in dash_refine_keys:\n",
    "    dvalues = keywords_dash_refine[dkey].split(\"; \")\n",
    "    \n",
    "    dvalues_abb = []\n",
    "    dvalues_rms = []\n",
    "    for dvalue in dvalues:\n",
    "        dvalue_rem = deepcopy(dvalue)\n",
    "        for akey in abb_refine_keys:\n",
    "            if dvalue == akey.lower():\n",
    "                dvalues.remove(dvalue)\n",
    "                dvalues.append(keywords_abb_refine[akey])\n",
    "            else:\n",
    "                [dvalues_abb.append(keywords_abb_refine[d.upper()]) for d in dvalue.split(\"-\") if d == akey.lower()]\n",
    "                [dvalues_abb.append(keywords_abb_refine[d.upper()]) for d in dvalue.split(\"/\") if d == akey.lower()]\n",
    "\n",
    "    keywords_dash_refine2.update({dkey:dvalues+dvalues_abb})\n",
    "\n",
    "with open(\"keywords_dash_refine2.json\", \"w\") as j:\n",
    "    json.dump(keywords_dash_refine2, j, ensure_ascii=False, indent=2)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. [manual] keywords_dash2 refinement\n",
    "* incorrect abbs treatement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. apply keywords-dash on keywords_value\n",
    "* (1) replace keywords_dash_refine.keys() to keywords_dash_refine.values()\n",
    "* (2) if keywords_dash_refine2.key() has more than 2 elements, add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_dash_refine2.json\", \"r\") as j:\n",
    "    keywords_dash_refine2 = json.load(j)\n",
    "    \n",
    "\n",
    "keywords_value_ = []\n",
    "for kws in keywords_value:\n",
    "    dash_flag = 0\n",
    "    kws_ = []\n",
    "    for dkey in dash_refine_keys:\n",
    "        \n",
    "        if dkey in kws:\n",
    "            kws_ += [kws.replace(dkey, keywords_dash_refine[dkey])]\n",
    "            dash_flag += 1\n",
    "            \n",
    "            if len(keywords_dash_refine2[dkey]) > 1:\n",
    "                [kws_.append(v) for v in keywords_dash_refine2[dkey][1:]]\n",
    "                \n",
    "    if dash_flag == 0:\n",
    "        kws_ = [kws]\n",
    "    \n",
    "    keywords_value_.append(list(np.unique(kws_)))\n",
    "\n",
    "keywords_dict = dict(zip(list(keywords_dict.keys()), keywords_value_))\n",
    "with open(keyword_dict_name, \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. find and extract dash and abbriviations\n",
    "* unify in-fact same words, and create `keywords_dict_add.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 46s, sys: 78.2 ms, total: 6min 46s\n",
      "Wall time: 6min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "\n",
    "abb_refine_keys = [k.lower() for k in np.unique(list(keywords_abb_refine.keys()))]\n",
    "abb_refine_values = [v.lower() for v in np.unique(list(keywords_abb_refine.values()))]\n",
    "\n",
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)    \n",
    "keywords_value = list(keywords_dict.values())\n",
    "\n",
    "keywords_value_add = []\n",
    "for kw in keywords_value:\n",
    "    kw_0 = kw\n",
    "    kw_1 = list(itertools.chain.from_iterable([kw_.split() for kw_ in kw]))\n",
    "    kw_2 = list(itertools.chain.from_iterable([kw_.split(\"-\") for kw_ in kw]))\n",
    "    kw_3 = list(itertools.chain.from_iterable([kw_.split(\"/\") for kw_ in kw]))\n",
    "    \n",
    "    keywords_value_add_ = []\n",
    "    \n",
    "    # \"dash\"\n",
    "    for dash_keys in dash_refine_keys:\n",
    "        if dash_keys in kw_1:\n",
    "            [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "        \n",
    "        dash_keys_ = dash_keys.replace(\"-\", \"\")\n",
    "        if dash_keys_ in kw_1:\n",
    "            if not (dash_keys_ in abb_refine_keys): # original abbriviation should be kept\n",
    "                [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "\n",
    "        dash_keys_ = dash_keys.split(\"-\")\n",
    "        if not (dash_keys_ in abb_refine_keys): # original abbriviation should be kept\n",
    "            try:\n",
    "                tmp = [kw_1.index(dash_key_) for dash_key_ in dash_keys_]\n",
    "                if tmp == sorted(tmp) and (len(np.unique(tmp)) == len(tmp)):\n",
    "                    [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    tmp = [kw_2.index(dash_key_) for dash_key_ in dash_keys_]\n",
    "                    if tmp == sorted(tmp) and (len(np.unique(tmp)) == len(tmp)):\n",
    "                        [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    \n",
    "    # \"abb\"\n",
    "    for abb_vals in abb_refine_values:\n",
    "        if abb_vals in kw and \\\n",
    "           (abb_vals.replace(\" \", \"-\") not in keywords_value_add) and \\\n",
    "           (abb_vals.replace(\"-\", \" \") not in keywords_value_add):\n",
    "            keywords_value_add_.append(abb_vals)\n",
    "    \n",
    "    for abb_keys in abb_refine_keys:\n",
    "        if abb_keys in (kw_0 + kw_1 + kw_2 + kw_3):\n",
    "            keywords_value_add_.append(keywords_abb_refine[abb_keys.upper()])\n",
    "    \n",
    "    keywords_value_add.append(list(np.unique(keywords_value_add_)))\n",
    "\n",
    "# create \"additional\" keywords dictionary\n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, keywords_value_add))\n",
    "with open(\"keywords_dict_add_dashabb.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9. [manual] create dictionary for manual replacement.\n",
    "* `keywords_manual.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10. update manual change on keywords\n",
    "* Update `keywords_manual` on `keywords_dict` and `keywords_dict_add_dashabb`, save as `keywords_dict2.json` and `keywords_dict_add_dashabb2.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_manual.json\", \"r\") as j:\n",
    "    keywords_manual = json.load(j)\n",
    "    \n",
    "with open(\"keywords_dict_add_dashabb.json\", \"r\") as j:\n",
    "    keywords_dict_add_dashabb = json.load(j)\n",
    "    \n",
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_keys = list(keywords_manual.keys())\n",
    "dict_values = list(keywords_dict.values())\n",
    "add_dashabb_values = list(keywords_dict_add_dashabb.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values_update = []\n",
    "\n",
    "for values in dict_values:\n",
    "    value_new = []\n",
    "    for value in values:\n",
    "        for mkey in manual_keys:\n",
    "            mkey_ = mkey.lower()\n",
    "            value = value.replace(mkey_, keywords_manual[mkey])\n",
    "        value_new.append(value)\n",
    "    dict_values_update.append(value_new)\n",
    "    \n",
    "keywords_dict = dict(zip(keywords_raw, dict_values_update))\n",
    "with open(\"keywords_dict2.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dashabb_values_update = []\n",
    "\n",
    "for values in add_dashabb_values:\n",
    "    value_new = []\n",
    "    for value in values:\n",
    "        for mkey in manual_keys:\n",
    "            mkey_ = mkey.lower()\n",
    "            value = value.replace(mkey_, keywords_manual[mkey])\n",
    "        value_new.append(value)\n",
    "    add_dashabb_values_update.append(value_new)\n",
    "    \n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, add_dashabb_values_update))\n",
    "with open(\"keywords_dict_add_dashabb2.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10. [manual] check still remaining same words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10224</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5291</th>\n",
       "      <td>generation</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8125</th>\n",
       "      <td>micro-grid</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11612</th>\n",
       "      <td>pv-system</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>energy-storage</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>irradiance</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5601</th>\n",
       "      <td>grid-connected</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10982</th>\n",
       "      <td>power-point</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>maximum-power-point</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>mpp tracker</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keywords  counts\n",
       "10224         photovoltaic    1731\n",
       "5291            generation     257\n",
       "8125            micro-grid     155\n",
       "11612            pv-system     112\n",
       "4287        energy-storage     108\n",
       "6915            irradiance     106\n",
       "5601        grid-connected      92\n",
       "10982          power-point      91\n",
       "7930   maximum-power-point      85\n",
       "8585           mpp tracker      85"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "merged = list(itertools.chain.from_iterable([dict_values_update, add_dashabb_values_update]))\n",
    "merged = list(itertools.chain.from_iterable(merged))\n",
    "merged_u = np.unique(merged, return_counts=True)\n",
    "\n",
    "words_u = merged_u[0]\n",
    "   \n",
    "\n",
    "df_words = pd.DataFrame({\"keywords\": merged_u[0],\n",
    "                         \"counts\": merged_u[1]\n",
    "                        }).sort_values(\"counts\", ascending=False)\n",
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10.(1) same words with and without dash(\"-\"): insert dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "keywords_dash_update = {}\n",
    "\n",
    "count = 0\n",
    "for word in words_u:\n",
    "    word_wo_dash = word.replace(\"-\", \"\")\n",
    "    if (word_wo_dash in words_u) and (\"-\" in word) :\n",
    "        count += 1\n",
    "        \n",
    "        det_0 = all([len(w) > 5 for w in word.split(\" \") if \"-\" in w])\n",
    "        det_1 = all([len(w) >= 2 for w in word.split(\"-\")]) \n",
    "#         print(count, word, det_0, det_1)\n",
    "        \n",
    "        if det_0 and det_1:\n",
    "            keywords_dash_update.update({word_wo_dash: word})\n",
    "\n",
    "# print(keywords_dash_update)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ac microgrid',\n",
       " 'antireflection',\n",
       " 'antireflection coating',\n",
       " 'asymmetrical fault',\n",
       " 'bidirectional',\n",
       " 'bidirectional converter',\n",
       " 'bidirectional dc/dc converter',\n",
       " 'bifacial',\n",
       " 'cogeneration',\n",
       " 'cosimulation']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(keywords_dash_update.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n"
     ]
    }
   ],
   "source": [
    "dash_update_keys = list(keywords_dash_update.keys())\n",
    "dict_values_update_ = deepcopy(dict_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(dict_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in dash_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                dict_values_update_[i][j] = keywords_dash_update[key]\n",
    "\n",
    "print(count)\n",
    "dict_values_update = deepcopy(dict_values_update_)\n",
    "\n",
    "keywords_dict = dict(zip(keywords_raw, dict_values_update_))\n",
    "with open(\"keywords_dict3.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "dash_update_keys = list(keywords_dash_update.keys())\n",
    "add_dashabb_values_update_ = deepcopy(add_dashabb_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(add_dashabb_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in dash_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                add_dashabb_values_update_[i][j] = keywords_dash_update[key]\n",
    "\n",
    "print(count)\n",
    "add_dashabb_values_update = deepcopy(add_dashabb_values_update_)\n",
    "\n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, add_dashabb_values_update_))\n",
    "with open(\"keywords_dict_add_dashabb3.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10153</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>generation</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8089</th>\n",
       "      <td>micro-grid</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>pv-system</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>energy-storage</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>irradiance</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5575</th>\n",
       "      <td>grid-connected</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10907</th>\n",
       "      <td>power-point</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>maximum-power-point</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8542</th>\n",
       "      <td>mpp tracker</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keywords  counts\n",
       "10153         photovoltaic    1731\n",
       "5265            generation     257\n",
       "8089            micro-grid     167\n",
       "11536            pv-system     112\n",
       "4266        energy-storage     108\n",
       "6884            irradiance     106\n",
       "5575        grid-connected      93\n",
       "10907          power-point      91\n",
       "7894   maximum-power-point      85\n",
       "8542           mpp tracker      85"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = list(itertools.chain.from_iterable([dict_values_update_, add_dashabb_values_update_]))\n",
    "merged = list(itertools.chain.from_iterable(merged))\n",
    "merged_u = np.unique(merged, return_counts=True)\n",
    "\n",
    "words_u = merged_u[0]\n",
    "   \n",
    "\n",
    "df_words = pd.DataFrame({\"keywords\": merged_u[0],\n",
    "                         \"counts\": merged_u[1]\n",
    "                        }).sort_values(\"counts\", ascending=False)\n",
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10.(2) same words with and without space(\" \"): remove space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "keywords_space_update = {}\n",
    "\n",
    "count = 0\n",
    "for word in words_u:\n",
    "    word_wo_space = word.replace(\" \", \"\")\n",
    "    if (word_wo_space in words_u) and (\" \" in word) :\n",
    "        count += 1\n",
    "        \n",
    "        det_0 = all([len(w) > 5 for w in word.split(\" \") if \"-\" in w])\n",
    "        det_1 = all([len(w) >= 2 for w in word.split(\"-\")]) \n",
    "#         print(count, word, det_0, det_1)\n",
    "        \n",
    "        if det_0 and det_1:\n",
    "            keywords_space_update.update({word: word_wo_space})\n",
    "\n",
    "# print(keywords_space_update)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto regressive': 'autoregressive',\n",
       " 'back propagation': 'backpropagation',\n",
       " 'co2 emission': 'co2emission',\n",
       " 'dig silent': 'digsilent',\n",
       " 'energy plan': 'energyplan',\n",
       " 'energy plus': 'energyplus',\n",
       " 'global grid': 'globalgrid',\n",
       " 'hydro power': 'hydropower',\n",
       " 'iec 61850': 'iec61850',\n",
       " 'lab view': 'labview',\n",
       " 'levenberg- marquardt': 'levenberg-marquardt',\n",
       " 'light gbm': 'lightgbm',\n",
       " 'mat lab': 'matlab',\n",
       " 'matlab / simulink': 'matlab/simulink',\n",
       " 'matlab/ simulink': 'matlab/simulink',\n",
       " 'micro controller': 'microcontroller',\n",
       " 'micro converter': 'microconverter',\n",
       " 'micro generation': 'microgeneration',\n",
       " 'micro source': 'microsource',\n",
       " 'nano fluid': 'nanofluid',\n",
       " 'p o': 'po',\n",
       " 'perturb & observe': 'perturb&observe',\n",
       " 'photo voltaic': 'photovoltaic',\n",
       " 'photovoltaic/ thermal': 'photovoltaic/thermal',\n",
       " 'power hardware-in-the-loop': 'powerhardware-in-the-loop',\n",
       " 'ret screen': 'retscreen',\n",
       " 'smart grid': 'smartgrid'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_space_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "with open(\"keywords_dict3.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)    \n",
    "    \n",
    "space_update_keys = list(keywords_space_update.keys())\n",
    "dict_values_update_ = deepcopy(dict_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(dict_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in space_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                dict_values_update_[i][j] = keywords_space_update[key]\n",
    "\n",
    "print(count)\n",
    "dict_values_update = deepcopy(dict_values_update_)\n",
    "\n",
    "keywords_dict = dict(zip(keywords_raw, dict_values_update_))\n",
    "with open(\"keywords_dict3.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open(\"keywords_dict_add_dashabb3.json\", \"r\") as j:\n",
    "    keywords_dict_dashabb3 = json.load(j)  \n",
    "\n",
    "dash_update_keys = list(keywords_dash_update.keys())\n",
    "add_dashabb_values_update_ = deepcopy(add_dashabb_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(add_dashabb_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in dash_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                add_dashabb_values_update_[i][j] = keywords_dash_update[key]\n",
    "\n",
    "print(count)\n",
    "add_dashabb_values_update = deepcopy(add_dashabb_values_update_)\n",
    "\n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, add_dashabb_values_update_))\n",
    "with open(\"keywords_dict_add_dashabb3.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10133</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5260</th>\n",
       "      <td>generation</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8072</th>\n",
       "      <td>micro-grid</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11514</th>\n",
       "      <td>pv-system</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>energy-storage</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6876</th>\n",
       "      <td>irradiance</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>grid-connected</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>power-point</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7881</th>\n",
       "      <td>maximum-power-point</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8525</th>\n",
       "      <td>mpp tracker</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keywords  counts\n",
       "10133         photovoltaic    1736\n",
       "5260            generation     257\n",
       "8072            micro-grid     167\n",
       "11514            pv-system     112\n",
       "4261        energy-storage     108\n",
       "6876            irradiance     106\n",
       "5569        grid-connected      93\n",
       "10885          power-point      91\n",
       "7881   maximum-power-point      85\n",
       "8525           mpp tracker      85"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = list(itertools.chain.from_iterable([dict_values_update_, add_dashabb_values_update_]))\n",
    "merged = list(itertools.chain.from_iterable(merged))\n",
    "merged_u = np.unique(merged, return_counts=True)\n",
    "\n",
    "words_u = merged_u[0]\n",
    "   \n",
    "\n",
    "df_words = pd.DataFrame({\"keywords\": merged_u[0],\n",
    "                         \"counts\": merged_u[1]\n",
    "                        }).sort_values(\"counts\", ascending=False)\n",
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10.(3) same words with dash(\"-\") and space(\" \"): insert dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n"
     ]
    }
   ],
   "source": [
    "keywords_dash_update = {}\n",
    "\n",
    "count = 0\n",
    "for word in words_u:\n",
    "    word_wo_dash = word.replace(\"-\", \" \")\n",
    "    if (word_wo_dash in words_u) and (\"-\" in word) :\n",
    "        count += 1\n",
    "        \n",
    "        det_0 = all([len(w) > 5 for w in word.split(\" \") if \"-\" in w])\n",
    "        det_1 = all([len(w) >= 2 for w in word.split(\"-\")]) \n",
    "#         print(count, word, det_0, det_1)\n",
    "        \n",
    "        if det_0 and det_1:\n",
    "            keywords_dash_update.update({word_wo_dash: word})\n",
    "\n",
    "# print(keywords_dash_update)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent based model',\n",
       " 'agent based modeling',\n",
       " 'air conditioning',\n",
       " 'amorphous silicon',\n",
       " 'arc flash',\n",
       " 'back to back converter',\n",
       " 'black box model',\n",
       " 'black box modeling',\n",
       " 'black start',\n",
       " 'boost converter']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(keywords_dash_update.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n"
     ]
    }
   ],
   "source": [
    "with open(\"keywords_dict3.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)    \n",
    "    \n",
    "dash_update_keys = list(keywords_dash_update.keys())\n",
    "dict_values_update_ = deepcopy(dict_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(dict_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in dash_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                dict_values_update_[i][j] = keywords_dash_update[key]\n",
    "\n",
    "print(count)\n",
    "dict_values_update = deepcopy(dict_values_update_)\n",
    "\n",
    "keywords_dict = dict(zip(keywords_raw, dict_values_update_))\n",
    "with open(\"keywords_dict4.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n"
     ]
    }
   ],
   "source": [
    "with open(\"keywords_dict_add_dashabb3.json\", \"r\") as j:\n",
    "    keywords_dict_dashabb3 = json.load(j)  \n",
    "\n",
    "dash_update_keys = list(keywords_dash_update.keys())\n",
    "add_dashabb_values_update_ = deepcopy(add_dashabb_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(add_dashabb_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in dash_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                add_dashabb_values_update_[i][j] = keywords_dash_update[key]\n",
    "\n",
    "print(count)\n",
    "add_dashabb_values_update = deepcopy(add_dashabb_values_update_)\n",
    "\n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, add_dashabb_values_update_))\n",
    "with open(\"keywords_dict_add_dashabb4.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9924</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>generation</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7901</th>\n",
       "      <td>micro-grid</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>energy-storage</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11270</th>\n",
       "      <td>pv-system</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7715</th>\n",
       "      <td>maximum-power-point</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>irradiance</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10144</th>\n",
       "      <td>photovoltaic-thermal</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13329</th>\n",
       "      <td>solar-photovoltaic</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>grid-connected</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   keywords  counts\n",
       "9924           photovoltaic    1736\n",
       "5159             generation     257\n",
       "7901             micro-grid     176\n",
       "4187         energy-storage     120\n",
       "11270             pv-system     118\n",
       "7715    maximum-power-point     113\n",
       "6737             irradiance     106\n",
       "10144  photovoltaic-thermal     103\n",
       "13329    solar-photovoltaic      96\n",
       "5450         grid-connected      96"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = list(itertools.chain.from_iterable([dict_values_update, add_dashabb_values_update]))\n",
    "merged = list(itertools.chain.from_iterable(merged))\n",
    "merged_u = np.unique(merged, return_counts=True)\n",
    "\n",
    "words_u = merged_u[0]\n",
    "   \n",
    "\n",
    "df_words = pd.DataFrame({\"keywords\": merged_u[0],\n",
    "                         \"counts\": merged_u[1]\n",
    "                        }).sort_values(\"counts\", ascending=False)\n",
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10.(4) apply `unique()` on values\n",
    "* save results in `keywords_dict_u.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_dict4.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)   \n",
    "dict_values = list(keywords_dict.values())\n",
    "dict_keys = list(keywords_dict.keys())\n",
    "\n",
    "with open(\"keywords_dict_add_dashabb4.json\", \"r\") as j:\n",
    "    keywords_dict_dashabb = json.load(j)  \n",
    "dict_dashabb_values = list(keywords_dict_dashabb.values())\n",
    "\n",
    "value_unique = []\n",
    "for i, (dict_value, dashabb_value) in enumerate(zip(dict_values, dict_dashabb_values)):\n",
    "    v1 = list(itertools.chain.from_iterable([v.split(\"; \") for v in dict_value]))\n",
    "    v2 = list(itertools.chain.from_iterable([v.split(\"; \") for v in dashabb_value]))\n",
    "    value_unique.append(list(set(v1 + v2)))\n",
    "    \n",
    "keywords_dict_u = dict(zip(keywords_raw, value_unique))\n",
    "with open(\"keywords_dict_u.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_u, j, ensure_ascii=False, indent=2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11. importing \"knowledge\"\n",
    "#### 3.11.(1) create knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_knowledge_init = {\n",
    "    \"statistics\": [\n",
    "        \"analysis of variance\", \n",
    "        \"autoregressive integrated moving average\", \n",
    "        \"autoregressive moving average\", \n",
    "        \"markov\", \n",
    "        \"kalman\", \n",
    "        \"bayes\", \n",
    "        \"gaussian process\", \n",
    "        \"autoregression\", \n",
    "        \"autoregressive\", \n",
    "        \"probabilistic\", \n",
    "        \"k-fold\", \n",
    "        \"ensemble\", \n",
    "        \"kruskal-wallis\" \n",
    "    ], \n",
    "\n",
    "    \"method\": [\n",
    "        \"taguchi\", \n",
    "        \"averaging point method\", \n",
    "        \"describing function method\", \n",
    "        \"finite difference time domain method\", \n",
    "        \"finite element method\", \n",
    "        \"group method of data handling\", \n",
    "        \"incremental conductance method\", \n",
    "        \"match evaluation method\", \n",
    "        \"multiple-shifted-frequency method\", \n",
    "        \"numerical method\", \n",
    "        \"oblique asymptote method\", \n",
    "        \"response surface methodology\", \n",
    "        \"steepest descent method\", \n",
    "        \"nelder-mead\", \n",
    "        \"newton-raphson\", \n",
    "        \"runge-kutta\", \n",
    "        \"conditional interpolation\", \n",
    "        \"expectation-maximization\", \n",
    "        \"dynamic simulation\", \n",
    "        \"empirical\", \n",
    "        \"emulation\", \n",
    "        \"gradient descent\", \n",
    "        \"algorithm\" \n",
    "    ],\n",
    "\n",
    "    \"metric\": [\n",
    "        \"mean square error\", \n",
    "        \"mean absolute error\", \n",
    "        \"mean absolute percentage error\", \n",
    "        \"cross-entropy\", \n",
    "        \"least-square\",\n",
    "        \"mean bias error\"\n",
    "    ], \n",
    "\n",
    "    \"machine learning\": [\n",
    "        \"clustering\", \n",
    "        \"regression\", \n",
    "        \"classification\", \n",
    "        \"dimension reduction\", \n",
    "        \"reinforcement learning\", \n",
    "        \"ensemble\", \n",
    "        \"natural language processing\"\n",
    "    ], \n",
    "\n",
    "    \"doc2vec\":[\n",
    "        \"natural language processing\"\n",
    "    ], \n",
    "\n",
    "    \"reinforcement learning\": [\n",
    "        \"sarsa\", \n",
    "        \"markov\"\n",
    "    ],\n",
    "    \n",
    "    \"ensemble\": [\n",
    "        \"random forest\", \n",
    "        \"adaboost\",\n",
    "        \"bagging\",\n",
    "        \"bootstrap\", \n",
    "        \"lightgbm\", \n",
    "        \"xgboost\"\n",
    "    ], \n",
    "    \n",
    "    \"data\": [\n",
    "        \"database\",\n",
    "        \"data acquisition\",\n",
    "        \"data-mining\",\n",
    "        \"data-driven\",\n",
    "        \"data-based\"\n",
    "    ],\n",
    "    \n",
    "    \"database\": [\n",
    "        \"mapreduce\",\n",
    "        \"sql\",\n",
    "        \"hadoop\"\n",
    "    ],\n",
    "    \n",
    "    \"classification\": [\n",
    "        \"k-nearest neighbor\",\n",
    "        \"support vector machine\",\n",
    "        \"neural network\"\n",
    "    ],\n",
    "    \n",
    "    \"regression\": [ \n",
    "        \"k-nearest neighbor\",\n",
    "        \"support vector machine\",\n",
    "        \"neural network\",\n",
    "        \"ridge\",\n",
    "        \"lasso\",\n",
    "        \"autoregressive\",\n",
    "        \"support vector regression\"\n",
    "    ],\n",
    "    \n",
    "    \"clustering\": [\n",
    "        \"k means\", \n",
    "        \"dbscan\"\n",
    "    ],\n",
    "    \n",
    "    \"neural network\": [\n",
    "        \"artificial neural network\",\n",
    "        \"learning vector quantization\",\n",
    "        \"recurrent neural network\",\n",
    "        \"convolutional neural network\",\n",
    "        \"autoencoder\",\n",
    "        \"adaline\",\n",
    "        \"artificial neural fuzzy inference system\",\n",
    "        \"elman\",\n",
    "        \"attention\",\n",
    "        \"extreme learning machine\",\n",
    "        \"multilayer perceptron\",\n",
    "    ],\n",
    "    \n",
    "    \"recurrent neural network\": [\n",
    "        \"long short-term memory\",\n",
    "        \"boltzmann machine\",\n",
    "    ],\n",
    "    \n",
    "    \"convolutional neural network\": [\n",
    "        \"googlenet\"\n",
    "    ], \n",
    "\n",
    "    \"dimension reduction\": [\n",
    "        \"principal component analysis\",\n",
    "        \"factor analysis\",\n",
    "        \"autoencoder\"\n",
    "    ],\n",
    "    \n",
    "    \"algorithm\": [\n",
    "        \"agent-based\",\n",
    "        \"ant colony\", \n",
    "        \"ant lion\", \n",
    "        \"artificial bee colony\", \n",
    "        \"artificial fish swarm\", \n",
    "        \"backtracking search\", \n",
    "        \"bacterial foraging\", \n",
    "        \"bat\", \n",
    "        \"bee pollinator\", \n",
    "        \"binary search\", \n",
    "        \"bio-inspired\", \n",
    "        \"bucket elimination\", \n",
    "        \"crow search\", \n",
    "        \"elite retention\", \n",
    "        \"evolutionary\", \n",
    "        \"firefly\", \n",
    "        \"fireworks explosion\", \n",
    "        \"flower pollination\", \n",
    "        \"fruitfly\", \n",
    "        \"genetic algorithm\", \n",
    "        \"golden section\", \n",
    "        \"grasshopper\", \n",
    "        \"gravity search\", \n",
    "        \"grey wolf\", \n",
    "        \"imperialist competition\", \n",
    "        \"jaya\", \n",
    "        \"leapfrog\", \n",
    "        \"honey bee mating\", \n",
    "        \"interior search\", \n",
    "        \"invasive weed\", \n",
    "        \"elephant herding\", \n",
    "        \"particle swarm\", \n",
    "        \"pattern search\", \n",
    "        \"perturb and observe\", \n",
    "        \"shuffled frog leaping\", \n",
    "        \"versatile threshold\", \n",
    "        \"monte carlo\", \n",
    "        \"rule-based\", \n",
    "        \"dynamic programming\"\n",
    "    ],\n",
    "    \n",
    "    \"dynamic programming\": [\n",
    "        \"reinforcement learning\"\n",
    "    ],\n",
    "    \n",
    "    \"building-integrated\": [\n",
    "        \"bapv\",\n",
    "        \"bipv\",\n",
    "        \"biss\",\n",
    "        \"bispv\",\n",
    "        \"bispvt\",\n",
    "        \"bipv\",\n",
    "        \"bipvt\",\n",
    "        \"bipv/t\",\n",
    "        \"bist\",\n",
    "        \"bists\"\n",
    "    ],\n",
    "    \n",
    "    \"thermal\": [\n",
    "        \"bapvt\",\n",
    "        \"bapv/t\",\n",
    "        \"bispvt\",\n",
    "        \"bipvt\",\n",
    "        \"bipv/t\",\n",
    "        \"bist\",\n",
    "        \"bists\"\n",
    "    ],\n",
    "    \n",
    "    \"photovoltaic\": [\n",
    "        \"bapv\",\n",
    "        \"bispv\",\n",
    "        \"bispvt\",\n",
    "        \"bipv\",\n",
    "        \"biss\",\n",
    "        \"bipv\",\n",
    "        \"bipvt\",\n",
    "        \"bipv/t\",\n",
    "        \"bist\",\n",
    "        \"bists\",\n",
    "        \"agrivoltaic\"\n",
    "    ],\n",
    "    \n",
    "    \"agriculture\": [\n",
    "        \"agrivoltaic\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "with open(\"keywords_knowledge_init.json\", \"w\") as j:\n",
    "    json.dump(keywords_knowledge_init, j, ensure_ascii=False, indent=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_keys_init = list(keywords_knowledge_init.keys())\n",
    "kn_values_init = list(keywords_knowledge_init.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_values_all = list(itertools.chain.from_iterable(kn_values_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.11.(2) Create knowledge graph\n",
    "* apply DFS to create knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key(value):\n",
    "    result_key = []\n",
    "    for k in kn_keys_init:\n",
    "        if value in keywords_knowledge_init[k]:\n",
    "            result_key.append(k)\n",
    "            \n",
    "    return result_key\n",
    "\n",
    "def dfs(graph, start_node):\n",
    "    visit = []\n",
    "    stack = []\n",
    "    \n",
    "    stack.append(start_node)\n",
    "    \n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node not in visit:\n",
    "            visit.append(node)\n",
    "            stack.extend(find_key(node))\n",
    "    return visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_knowledge = {}\n",
    "for kn in kn_values_all:\n",
    "    value_add = dfs(keywords_knowledge_init, kn)\n",
    "    keywords_knowledge.update({kn: value_add})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysis of variance': ['analysis of variance', 'statistics'],\n",
       " 'autoregressive integrated moving average': ['autoregressive integrated moving average',\n",
       "  'statistics'],\n",
       " 'autoregressive moving average': ['autoregressive moving average',\n",
       "  'statistics'],\n",
       " 'markov': ['markov',\n",
       "  'reinforcement learning',\n",
       "  'dynamic programming',\n",
       "  'algorithm',\n",
       "  'method',\n",
       "  'machine learning',\n",
       "  'statistics'],\n",
       " 'kalman': ['kalman', 'statistics'],\n",
       " 'bayes': ['bayes', 'statistics'],\n",
       " 'gaussian process': ['gaussian process', 'statistics'],\n",
       " 'autoregression': ['autoregression', 'statistics'],\n",
       " 'autoregressive': ['autoregressive',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'statistics'],\n",
       " 'probabilistic': ['probabilistic', 'statistics'],\n",
       " 'k-fold': ['k-fold', 'statistics'],\n",
       " 'ensemble': ['ensemble', 'machine learning', 'statistics'],\n",
       " 'kruskal-wallis': ['kruskal-wallis', 'statistics'],\n",
       " 'taguchi': ['taguchi', 'method'],\n",
       " 'averaging point method': ['averaging point method', 'method'],\n",
       " 'describing function method': ['describing function method', 'method'],\n",
       " 'finite difference time domain method': ['finite difference time domain method',\n",
       "  'method'],\n",
       " 'finite element method': ['finite element method', 'method'],\n",
       " 'group method of data handling': ['group method of data handling', 'method'],\n",
       " 'incremental conductance method': ['incremental conductance method',\n",
       "  'method'],\n",
       " 'match evaluation method': ['match evaluation method', 'method'],\n",
       " 'multiple-shifted-frequency method': ['multiple-shifted-frequency method',\n",
       "  'method'],\n",
       " 'numerical method': ['numerical method', 'method'],\n",
       " 'oblique asymptote method': ['oblique asymptote method', 'method'],\n",
       " 'response surface methodology': ['response surface methodology', 'method'],\n",
       " 'steepest descent method': ['steepest descent method', 'method'],\n",
       " 'nelder-mead': ['nelder-mead', 'method'],\n",
       " 'newton-raphson': ['newton-raphson', 'method'],\n",
       " 'runge-kutta': ['runge-kutta', 'method'],\n",
       " 'conditional interpolation': ['conditional interpolation', 'method'],\n",
       " 'expectation-maximization': ['expectation-maximization', 'method'],\n",
       " 'dynamic simulation': ['dynamic simulation', 'method'],\n",
       " 'empirical': ['empirical', 'method'],\n",
       " 'emulation': ['emulation', 'method'],\n",
       " 'gradient descent': ['gradient descent', 'method'],\n",
       " 'algorithm': ['algorithm', 'method'],\n",
       " 'mean square error': ['mean square error', 'metric'],\n",
       " 'mean absolute error': ['mean absolute error', 'metric'],\n",
       " 'mean absolute percentage error': ['mean absolute percentage error',\n",
       "  'metric'],\n",
       " 'cross-entropy': ['cross-entropy', 'metric'],\n",
       " 'least-square': ['least-square', 'metric'],\n",
       " 'mean bias error': ['mean bias error', 'metric'],\n",
       " 'clustering': ['clustering', 'machine learning'],\n",
       " 'regression': ['regression', 'machine learning'],\n",
       " 'classification': ['classification', 'machine learning'],\n",
       " 'dimension reduction': ['dimension reduction', 'machine learning'],\n",
       " 'reinforcement learning': ['reinforcement learning',\n",
       "  'dynamic programming',\n",
       "  'algorithm',\n",
       "  'method',\n",
       "  'machine learning'],\n",
       " 'natural language processing': ['natural language processing',\n",
       "  'doc2vec',\n",
       "  'machine learning'],\n",
       " 'sarsa': ['sarsa',\n",
       "  'reinforcement learning',\n",
       "  'dynamic programming',\n",
       "  'algorithm',\n",
       "  'method',\n",
       "  'machine learning'],\n",
       " 'random forest': ['random forest',\n",
       "  'ensemble',\n",
       "  'machine learning',\n",
       "  'statistics'],\n",
       " 'adaboost': ['adaboost', 'ensemble', 'machine learning', 'statistics'],\n",
       " 'bagging': ['bagging', 'ensemble', 'machine learning', 'statistics'],\n",
       " 'bootstrap': ['bootstrap', 'ensemble', 'machine learning', 'statistics'],\n",
       " 'lightgbm': ['lightgbm', 'ensemble', 'machine learning', 'statistics'],\n",
       " 'xgboost': ['xgboost', 'ensemble', 'machine learning', 'statistics'],\n",
       " 'database': ['database', 'data'],\n",
       " 'data acquisition': ['data acquisition', 'data'],\n",
       " 'data-mining': ['data-mining', 'data'],\n",
       " 'data-driven': ['data-driven', 'data'],\n",
       " 'data-based': ['data-based', 'data'],\n",
       " 'mapreduce': ['mapreduce', 'database', 'data'],\n",
       " 'sql': ['sql', 'database', 'data'],\n",
       " 'hadoop': ['hadoop', 'database', 'data'],\n",
       " 'k-nearest neighbor': ['k-nearest neighbor',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'support vector machine': ['support vector machine',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'neural network': ['neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'ridge': ['ridge', 'regression', 'machine learning'],\n",
       " 'lasso': ['lasso', 'regression', 'machine learning'],\n",
       " 'support vector regression': ['support vector regression',\n",
       "  'regression',\n",
       "  'machine learning'],\n",
       " 'k means': ['k means', 'clustering', 'machine learning'],\n",
       " 'dbscan': ['dbscan', 'clustering', 'machine learning'],\n",
       " 'artificial neural network': ['artificial neural network',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'learning vector quantization': ['learning vector quantization',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'recurrent neural network': ['recurrent neural network',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'convolutional neural network': ['convolutional neural network',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'autoencoder': ['autoencoder',\n",
       "  'dimension reduction',\n",
       "  'machine learning',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'classification'],\n",
       " 'adaline': ['adaline',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'artificial neural fuzzy inference system': ['artificial neural fuzzy inference system',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'elman': ['elman',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'attention': ['attention',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'extreme learning machine': ['extreme learning machine',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'multilayer perceptron': ['multilayer perceptron',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'long short-term memory': ['long short-term memory',\n",
       "  'recurrent neural network',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'boltzmann machine': ['boltzmann machine',\n",
       "  'recurrent neural network',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'googlenet': ['googlenet',\n",
       "  'convolutional neural network',\n",
       "  'neural network',\n",
       "  'regression',\n",
       "  'machine learning',\n",
       "  'classification'],\n",
       " 'principal component analysis': ['principal component analysis',\n",
       "  'dimension reduction',\n",
       "  'machine learning'],\n",
       " 'factor analysis': ['factor analysis',\n",
       "  'dimension reduction',\n",
       "  'machine learning'],\n",
       " 'agent-based': ['agent-based', 'algorithm', 'method'],\n",
       " 'ant colony': ['ant colony', 'algorithm', 'method'],\n",
       " 'ant lion': ['ant lion', 'algorithm', 'method'],\n",
       " 'artificial bee colony': ['artificial bee colony', 'algorithm', 'method'],\n",
       " 'artificial fish swarm': ['artificial fish swarm', 'algorithm', 'method'],\n",
       " 'backtracking search': ['backtracking search', 'algorithm', 'method'],\n",
       " 'bacterial foraging': ['bacterial foraging', 'algorithm', 'method'],\n",
       " 'bat': ['bat', 'algorithm', 'method'],\n",
       " 'bee pollinator': ['bee pollinator', 'algorithm', 'method'],\n",
       " 'binary search': ['binary search', 'algorithm', 'method'],\n",
       " 'bio-inspired': ['bio-inspired', 'algorithm', 'method'],\n",
       " 'bucket elimination': ['bucket elimination', 'algorithm', 'method'],\n",
       " 'crow search': ['crow search', 'algorithm', 'method'],\n",
       " 'elite retention': ['elite retention', 'algorithm', 'method'],\n",
       " 'evolutionary': ['evolutionary', 'algorithm', 'method'],\n",
       " 'firefly': ['firefly', 'algorithm', 'method'],\n",
       " 'fireworks explosion': ['fireworks explosion', 'algorithm', 'method'],\n",
       " 'flower pollination': ['flower pollination', 'algorithm', 'method'],\n",
       " 'fruitfly': ['fruitfly', 'algorithm', 'method'],\n",
       " 'genetic algorithm': ['genetic algorithm', 'algorithm', 'method'],\n",
       " 'golden section': ['golden section', 'algorithm', 'method'],\n",
       " 'grasshopper': ['grasshopper', 'algorithm', 'method'],\n",
       " 'gravity search': ['gravity search', 'algorithm', 'method'],\n",
       " 'grey wolf': ['grey wolf', 'algorithm', 'method'],\n",
       " 'imperialist competition': ['imperialist competition', 'algorithm', 'method'],\n",
       " 'jaya': ['jaya', 'algorithm', 'method'],\n",
       " 'leapfrog': ['leapfrog', 'algorithm', 'method'],\n",
       " 'honey bee mating': ['honey bee mating', 'algorithm', 'method'],\n",
       " 'interior search': ['interior search', 'algorithm', 'method'],\n",
       " 'invasive weed': ['invasive weed', 'algorithm', 'method'],\n",
       " 'elephant herding': ['elephant herding', 'algorithm', 'method'],\n",
       " 'particle swarm': ['particle swarm', 'algorithm', 'method'],\n",
       " 'pattern search': ['pattern search', 'algorithm', 'method'],\n",
       " 'perturb and observe': ['perturb and observe', 'algorithm', 'method'],\n",
       " 'shuffled frog leaping': ['shuffled frog leaping', 'algorithm', 'method'],\n",
       " 'versatile threshold': ['versatile threshold', 'algorithm', 'method'],\n",
       " 'monte carlo': ['monte carlo', 'algorithm', 'method'],\n",
       " 'rule-based': ['rule-based', 'algorithm', 'method'],\n",
       " 'dynamic programming': ['dynamic programming', 'algorithm', 'method'],\n",
       " 'bapv': ['bapv', 'photovoltaic', 'building-integrated'],\n",
       " 'bipv': ['bipv', 'photovoltaic', 'building-integrated'],\n",
       " 'biss': ['biss', 'photovoltaic', 'building-integrated'],\n",
       " 'bispv': ['bispv', 'photovoltaic', 'building-integrated'],\n",
       " 'bispvt': ['bispvt', 'photovoltaic', 'thermal', 'building-integrated'],\n",
       " 'bipvt': ['bipvt', 'photovoltaic', 'thermal', 'building-integrated'],\n",
       " 'bipv/t': ['bipv/t', 'photovoltaic', 'thermal', 'building-integrated'],\n",
       " 'bist': ['bist', 'photovoltaic', 'thermal', 'building-integrated'],\n",
       " 'bists': ['bists', 'photovoltaic', 'thermal', 'building-integrated'],\n",
       " 'bapvt': ['bapvt', 'thermal'],\n",
       " 'bapv/t': ['bapv/t', 'thermal'],\n",
       " 'agrivoltaic': ['agrivoltaic', 'agriculture', 'photovoltaic']}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.11.(3) Create knowledge dictionary and save\n",
    "* save as `keywords_dict_kn.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values_kn = []\n",
    "for values in dict_values:\n",
    "    dict_values_kn_ = []\n",
    "\n",
    "    for value in values:\n",
    "        for kn_key in kn_values_all:        \n",
    "            # multiple words (ex. monte carlo, rule-based):\n",
    "            if len(kn_key.replace(\"-\", \" \").split(\" \")) > 1:\n",
    "                kn_key0 = \"-\".join(kn_key.replace(\"-\",\" \").replace(\"_\", \" \").split(\" \"))\n",
    "                if kn_key0 in value.replace(\" \",\"-\").replace(\"_\", \"-\"):\n",
    "                    dict_values_kn_.extend(keywords_knowledge[kn_key])\n",
    "                \n",
    "            # single word (ex. ensemble)\n",
    "            else:\n",
    "                if kn_key in value.replace(\"-\",\" \").split(\" \"):\n",
    "                    dict_values_kn_.extend(keywords_knowledge[kn_key])\n",
    "            \n",
    "    dict_values_kn.append(list(set(dict_values_kn_)))\n",
    "\n",
    "\n",
    "keywords_dict_kn = dict(zip(dict_keys, dict_values_kn))\n",
    "with open(\"keywords_dict_kn.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_kn, j, ensure_ascii=False, indent=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12. Update frequent values\n",
    "* \"frequent\" keywords should not be overlapped with \"dash-words\", \"abbriviations\" and \"knowledge graph\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.12.(1) load data\n",
    "* refine words in (dict values - dashabb - knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)   \n",
    "dict_values = list(keywords_dict.values())\n",
    "dict_keys = list(keywords_dict.keys())\n",
    "\n",
    "with open(\"keywords_abb_refine.json\", \"r\") as j:\n",
    "    keywords_dict_abb = json.load(j)  \n",
    "dict_abb_keys = list(keywords_dict_abb.keys())\n",
    "dict_abb_values = list(keywords_dict_abb.values())\n",
    "\n",
    "with open(\"keywords_dash_refine2.json\", \"r\") as j:\n",
    "    keywords_dict_dash = json.load(j)  \n",
    "dict_dash_values = list(keywords_dict_dash.values())\n",
    "dict_dash_values_u = list(itertools.chain.from_iterable(dict_dash_values))\n",
    "\n",
    "with open(\"keywords_dict_kn.json\", \"r\") as j:\n",
    "    keywords_dict_kn = json.load(j)  \n",
    "dict_kn_values = list(keywords_dict_kn.values())\n",
    "\n",
    "with open(\"keywords_manual.json\", \"r\") as j:\n",
    "    keywords_dict_man = json.load(j)  \n",
    "dict_man_keys = list(keywords_dict_man.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ac impedance model'],\n",
       " ['ac micro-grid'],\n",
       " ['ac microgrid'],\n",
       " ['ac microgrids'],\n",
       " ['ac microgrids'],\n",
       " ['ac modelling'],\n",
       " ['ac module'],\n",
       " ['ac shunted system and matlab/simulink'],\n",
       " ['ac small-signal model'],\n",
       " ['ac-dc microgrid'],\n",
       " ['ac-dc power converter'],\n",
       " ['ac-stacked pv inverter'],\n",
       " ['ac-stacked pv inverter'],\n",
       " ['ac-stacked inverter'],\n",
       " ['ac-stacked inverter'],\n",
       " ['ac-stacked photovoltaic inverter'],\n",
       " ['ac/dc grid'],\n",
       " ['ac/dc hybrid microgrid'],\n",
       " ['ac/dc hybrid system'],\n",
       " ['ac algorithm']]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_values[90:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['boost-self'],\n",
       " ['boost-type'],\n",
       " ['bos-costs'],\n",
       " ['bottom-up'],\n",
       " ['bouguer-lambert'],\n",
       " ['box-behnken'],\n",
       " ['artificial neural network'],\n",
       " ['bpfpa-bee'],\n",
       " ['branch-and-bound'],\n",
       " ['bridge-link'],\n",
       " ['bridge-linked'],\n",
       " ['brute-force'],\n",
       " ['buck-boost'],\n",
       " ['building-applied'],\n",
       " ['building-integrated'],\n",
       " ['building-integration'],\n",
       " ['built-in'],\n",
       " ['buk-boost'],\n",
       " ['buoyancy-induced'],\n",
       " ['c-c']]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_dash_values[90:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.12.(2) extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_freq = []\n",
    "for values in dict_values:\n",
    "    value_net = []\n",
    "    dash_net = []\n",
    "    abb_net = []\n",
    "    \n",
    "    values_summary = []\n",
    "    for value in values:\n",
    "        \n",
    "        # abbriviation detection\n",
    "        value_ = value.replace(\" \", \"-\").replace(\"/\", \"-\")\n",
    "        value_split = value_.split(\"-\")\n",
    "        value_assemble = []\n",
    "        abb_removal = []\n",
    "        for value_s in value_split:\n",
    "            for abb in dict_abb_keys:\n",
    "                abb_ = abb.lower()\n",
    "                if abb_ in value_split and keywords_dict_abb[abb] not in abb_net:\n",
    "                    abb_net.append(keywords_dict_abb[abb])\n",
    "                    abb_removal.append(abb_)\n",
    "                \n",
    "        value_filtered = \"-\".join([\"\" if v in abb_removal else v for v in value_split]).rstrip(\"-\").lstrip(\"-\")\n",
    "        \n",
    "        # dash-words detection\n",
    "        dash_removal = []\n",
    "        for dash in dict_dash_values_u:\n",
    "            dash0 = dash.replace(\"-\", \"\")\n",
    "            dash1 = dash.replace(\"-\", \" \")\n",
    "            if (dash in value_filtered) and (dash not in dash_net) and len(dash)>=4:\n",
    "                dash_net.append(dash)\n",
    "                dash_removal.append(dash)\n",
    "            elif (dash0 in value_filtered) and (dash not in dash_net) and len(dash)>=4:\n",
    "                dash_net.append(dash)\n",
    "                dash_removal.append(dash0)\n",
    "            elif (dash1 in value_filtered) and (dash not in dash_net) and len(dash)>=4:\n",
    "                dash_net.append(dash)\n",
    "                dash_removal.append(dash1)\n",
    "        \n",
    "        for dash_rm in dash_removal:\n",
    "            value_filtered = value_filtered.replace(dash_rm, \"\")\n",
    "        \n",
    "        value_net.extend([v for v in value_filtered.split(\"-\") if len(v)>=2])\n",
    "    \n",
    "        # manual correction\n",
    "        value_sum = value_net + abb_net + dash_net\n",
    "        for i, v in enumerate(value_sum):\n",
    "            for mkey in dict_man_keys:\n",
    "                mkey_ = mkey.lower()\n",
    "                v_re = v.replace(mkey_, keywords_dict_man[mkey])\n",
    "                value_sum[i] = v_re\n",
    "        \n",
    "        values_summary.extend(list(set(value_sum)))\n",
    "    \n",
    "    values_freq.append(values_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dict_kw = dict(zip(dict_keys, values_freq))\n",
    "with open(\"keywords_dict_kw.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_kw, j, ensure_ascii=False, indent=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.12.(3) sort keywords with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>system</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>power</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>solar</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>energy</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>modeling</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>control</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>network</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>of</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>converter</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>analysis</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>algorithm</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gen-eration</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cell</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>inverter</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>prediction</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>thermal</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        keywords  counts\n",
       "0   photovoltaic    1829\n",
       "1         system    1233\n",
       "2          power     898\n",
       "3          model     864\n",
       "4          solar     848\n",
       "5         energy     755\n",
       "6       modeling     483\n",
       "7        control     454\n",
       "8            and     405\n",
       "9        network     329\n",
       "10            of     308\n",
       "11     converter     304\n",
       "12      analysis     297\n",
       "13        hybrid     294\n",
       "14     algorithm     293\n",
       "15   gen-eration     278\n",
       "16          cell     271\n",
       "17      inverter     269\n",
       "18    prediction     255\n",
       "19       thermal     249"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"keywords_dict_kw.json\", \"r\") as j:\n",
    "    keywords_dict_kw = json.load(j)  \n",
    "dict_kw_values = list(keywords_dict_kw.values())\n",
    "\n",
    "values_freq = list(itertools.chain.from_iterable(dict_kw_values))\n",
    "values_freq = np.unique(values_freq, return_counts=True)    \n",
    "\n",
    "df_words = pd.DataFrame({\"keywords\": values_freq[0],\n",
    "                         \"counts\": values_freq[1]\n",
    "                        }).sort_values(\"counts\", ascending=False).reset_index(drop=True)\n",
    "df_words.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.12.(4) apply 'stopwords'\n",
    "* remove meaningless words by 'stop words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>system</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>power</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>solar</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>energy</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>modeling</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>control</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>network</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>converter</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>analysis</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>algorithm</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gen-eration</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cell</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>inverter</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>prediction</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>thermal</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>optimization</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>battery</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        keywords  counts\n",
       "0   photovoltaic    1829\n",
       "1         system    1233\n",
       "2          power     898\n",
       "3          model     864\n",
       "4          solar     848\n",
       "5         energy     755\n",
       "6       modeling     483\n",
       "7        control     454\n",
       "9        network     329\n",
       "11     converter     304\n",
       "12      analysis     297\n",
       "13        hybrid     294\n",
       "14     algorithm     293\n",
       "15   gen-eration     278\n",
       "16          cell     271\n",
       "17      inverter     269\n",
       "18    prediction     255\n",
       "19       thermal     249\n",
       "20  optimization     245\n",
       "21       battery     207"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = [\"and\", \"of\", \"the\", \"to\", \"on\", \"in\", \"based\", \"base\"]\n",
    "\n",
    "for sword in stop_words:\n",
    "    idx = df_words[df_words[\"keywords\"] == sword ].index\n",
    "    df_words.drop(idx, axis=0, inplace=True)\n",
    "    \n",
    "for i in range(ord(\"a\"), ord(\"z\")+1):\n",
    "    idx = df_words[df_words[\"keywords\"] == chr(i)].index\n",
    "    df_words.drop(idx, axis=0, inplace=True)\n",
    "\n",
    "df_words.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.12.(3) check ranks of some important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([571]),)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df_words[\"keywords\"]==\"shadow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([37]),)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df_words[\"keywords\"]==\"building\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df_words[\"keywords\"]==\"based\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([161]),)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df_words[\"keywords\"]==\"building-integrated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['photovoltaic', 'system', 'power', 'model', 'solar', 'energy', 'modeling', 'control', 'network', 'converter', 'analysis', 'hybrid', 'algorithm', 'gen-eration', 'cell', 'inverter', 'prediction', 'thermal', 'optimization', 'battery', 'modelling', 'load', 'simulation', 'method', 'voltage', 'module', 'renewable', 'distributed', 'grid', 'wind', 'dynamic', 'forecasting', 'distribution', 'management', 'micro-grid']\n"
     ]
    }
   ],
   "source": [
    "words_single_1000 = df_words[\"keywords\"].iloc[:1000].tolist()\n",
    "print(words_single_1000[:35])\n",
    "# print(words_single_1200[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241]),)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df_words[\"counts\"]>30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.12.(4) extract top N words\n",
    "* extract words in top 1000\n",
    "* save the words in `keywords_dict_freq.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_dict_kw.json\", \"r\") as j:\n",
    "    keywords_dict_kw = json.load(j)  \n",
    "dict_kw_values = list(keywords_dict_kw.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19457\n"
     ]
    }
   ],
   "source": [
    "words_manual = []\n",
    "words_select = words_single_1000 + words_manual\n",
    "\n",
    "freq_values = []\n",
    "for values in dict_kw_values :\n",
    "    values_add = []\n",
    "    for w in words_select:\n",
    "        values_add.append([w for v in values if w in v.replace(\"-\", \" \").split(\" \")])\n",
    "    freq_values.append(list(set(itertools.chain.from_iterable(values_add))))\n",
    "\n",
    "print(len(freq_values))\n",
    "keywords_dict_freq = dict(zip(list(keywords_dict_kw.keys()), freq_values))\n",
    "with open(\"keywords_dict_freq.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_freq, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artificial', 'network', 'neural']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_dict_freq[\"artificial neural network\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keywords Summing up \n",
    "* `dict_u` + `dict_kw` + `dict_kn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_dict_freq.json\", \"r\") as j:\n",
    "    keywords_dict_freq = json.load(j)   \n",
    "dict_freq_values = list(keywords_dict_freq.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. gathering cleansed keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_keywords</th>\n",
       "      <th>1_unique</th>\n",
       "      <th>2_freq</th>\n",
       "      <th>3_knowledge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Economic benefits\"</td>\n",
       "      <td>[economic benefit]</td>\n",
       "      <td>[economic]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Finance lease\"</td>\n",
       "      <td>[finance lease]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Household distributed photovoltaic\"</td>\n",
       "      <td>[household distributed photovoltaic]</td>\n",
       "      <td>[photovoltaic, household, distributed]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"LV networks</td>\n",
       "      <td>[lv network]</td>\n",
       "      <td>[network]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Photovoltaic loan\"</td>\n",
       "      <td>[photovoltaic loan]</td>\n",
       "      <td>[photovoltaic]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>% free PSA (% FPSA)</td>\n",
       "      <td>[% free psa]</td>\n",
       "      <td>[free]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(EPV,Qd) operating points</td>\n",
       "      <td>[operating point]</td>\n",
       "      <td>[operating, point]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(PV) Photovoltaic panel</td>\n",
       "      <td>[photovoltaic panel]</td>\n",
       "      <td>[photovoltaic, panel]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(V-P) voltage power</td>\n",
       "      <td>[voltage power]</td>\n",
       "      <td>[voltage, power]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-Artificial-Neural-Network</td>\n",
       "      <td>[artificial neural network]</td>\n",
       "      <td>[artificial, network, neural]</td>\n",
       "      <td>[neural network, machine learning, classificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-Back-propagation-algorithm</td>\n",
       "      <td>[artificial neural network, artificial neural ...</td>\n",
       "      <td>[artificial, algorithm, network, neural]</td>\n",
       "      <td>[method, neural network, machine learning, alg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-DC/DC-boost-converter</td>\n",
       "      <td>[dc/dc-boost-converter]</td>\n",
       "      <td>[converter, boost, dc]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-Fuzzy-Logic-Controller</td>\n",
       "      <td>[fuzzy-logic-controller]</td>\n",
       "      <td>[controller, fuzzy]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-Hybrid-Neurone-Fuzzy</td>\n",
       "      <td>[hybrid-neurone-fuzzy]</td>\n",
       "      <td>[hybrid, fuzzy]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-Maximum-Power-Point-Tracker</td>\n",
       "      <td>[maximum-power-point-tracker, maximum-power-po...</td>\n",
       "      <td>[tracker, point, power, maximum, tracking]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0_keywords  \\\n",
       "0                    \"Economic benefits\"   \n",
       "1                        \"Finance lease\"   \n",
       "2   \"Household distributed photovoltaic\"   \n",
       "3                           \"LV networks   \n",
       "4                    \"Photovoltaic loan\"   \n",
       "5                    % free PSA (% FPSA)   \n",
       "6              (EPV,Qd) operating points   \n",
       "7                (PV) Photovoltaic panel   \n",
       "8                    (V-P) voltage power   \n",
       "9             -Artificial-Neural-Network   \n",
       "10           -Back-propagation-algorithm   \n",
       "11                -DC/DC-boost-converter   \n",
       "12               -Fuzzy-Logic-Controller   \n",
       "13                 -Hybrid-Neurone-Fuzzy   \n",
       "14          -Maximum-Power-Point-Tracker   \n",
       "\n",
       "                                             1_unique  \\\n",
       "0                                  [economic benefit]   \n",
       "1                                     [finance lease]   \n",
       "2                [household distributed photovoltaic]   \n",
       "3                                        [lv network]   \n",
       "4                                 [photovoltaic loan]   \n",
       "5                                        [% free psa]   \n",
       "6                                   [operating point]   \n",
       "7                                [photovoltaic panel]   \n",
       "8                                     [voltage power]   \n",
       "9                         [artificial neural network]   \n",
       "10  [artificial neural network, artificial neural ...   \n",
       "11                            [dc/dc-boost-converter]   \n",
       "12                           [fuzzy-logic-controller]   \n",
       "13                             [hybrid-neurone-fuzzy]   \n",
       "14  [maximum-power-point-tracker, maximum-power-po...   \n",
       "\n",
       "                                        2_freq  \\\n",
       "0                                   [economic]   \n",
       "1                                           []   \n",
       "2       [photovoltaic, household, distributed]   \n",
       "3                                    [network]   \n",
       "4                               [photovoltaic]   \n",
       "5                                       [free]   \n",
       "6                           [operating, point]   \n",
       "7                        [photovoltaic, panel]   \n",
       "8                             [voltage, power]   \n",
       "9                [artificial, network, neural]   \n",
       "10    [artificial, algorithm, network, neural]   \n",
       "11                      [converter, boost, dc]   \n",
       "12                         [controller, fuzzy]   \n",
       "13                             [hybrid, fuzzy]   \n",
       "14  [tracker, point, power, maximum, tracking]   \n",
       "\n",
       "                                          3_knowledge  \n",
       "0                                                  []  \n",
       "1                                                  []  \n",
       "2                                                  []  \n",
       "3                                                  []  \n",
       "4                                                  []  \n",
       "5                                                  []  \n",
       "6                                                  []  \n",
       "7                                                  []  \n",
       "8                                                  []  \n",
       "9   [neural network, machine learning, classificat...  \n",
       "10  [method, neural network, machine learning, alg...  \n",
       "11                                                 []  \n",
       "12                                                 []  \n",
       "13                                                 []  \n",
       "14                                                 []  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict_sum = pd.DataFrame({\"0_keywords\": dict_keys,\n",
    "                            \"1_unique\": dict_values,\n",
    "                            \"2_freq\": dict_freq_values,\n",
    "                            \"3_knowledge\": dict_kn_values\n",
    "                           })\n",
    "df_dict_sum.to_csv(\"keywords_summary.csv\", index=False)\n",
    "df_dict_sum.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. cleaning redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artificial', 'algorithm', 'network', 'neural']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_freq_values[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['method',\n",
       " 'neural network',\n",
       " 'machine learning',\n",
       " 'algorithm',\n",
       " 'classification',\n",
       " 'regression',\n",
       " 'artificial neural network']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_kn_values[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "kn_value = ['artificial neural network']\n",
    "freq_value = ['network', 'artificial']\n",
    "\n",
    "for kn in kn_value:\n",
    "    print(len(set(kn.split(\" \")) - set(freq_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_keywords</th>\n",
       "      <th>1_unique</th>\n",
       "      <th>2_freq1000</th>\n",
       "      <th>3_knowledge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Economic benefits\"</td>\n",
       "      <td>[economic benefit]</td>\n",
       "      <td>[economic]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Finance lease\"</td>\n",
       "      <td>[finance lease]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Household distributed photovoltaic\"</td>\n",
       "      <td>[household distributed photovoltaic]</td>\n",
       "      <td>[photovoltaic, household, distributed]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"LV networks</td>\n",
       "      <td>[lv network]</td>\n",
       "      <td>[network]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Photovoltaic loan\"</td>\n",
       "      <td>[photovoltaic loan]</td>\n",
       "      <td>[photovoltaic]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>% free PSA (% FPSA)</td>\n",
       "      <td>[% free psa]</td>\n",
       "      <td>[free]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(EPV,Qd) operating points</td>\n",
       "      <td>[operating point]</td>\n",
       "      <td>[operating, point]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(PV) Photovoltaic panel</td>\n",
       "      <td>[photovoltaic panel]</td>\n",
       "      <td>[photovoltaic, panel]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(V-P) voltage power</td>\n",
       "      <td>[voltage power]</td>\n",
       "      <td>[voltage, power]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-Artificial-Neural-Network</td>\n",
       "      <td>[artificial neural network]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[neural network, machine learning, classificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-Back-propagation-algorithm</td>\n",
       "      <td>[artificial neural network, artificial neural ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[method, neural network, machine learning, alg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-DC/DC-boost-converter</td>\n",
       "      <td>[dc/dc-boost-converter]</td>\n",
       "      <td>[converter, boost, dc]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-Fuzzy-Logic-Controller</td>\n",
       "      <td>[fuzzy-logic-controller]</td>\n",
       "      <td>[controller, fuzzy]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-Hybrid-Neurone-Fuzzy</td>\n",
       "      <td>[hybrid-neurone-fuzzy]</td>\n",
       "      <td>[hybrid, fuzzy]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-Maximum-Power-Point-Tracker</td>\n",
       "      <td>[maximum-power-point-tracker, maximum-power-po...</td>\n",
       "      <td>[maximum, point, tracking, tracker, power]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0_keywords  \\\n",
       "0                    \"Economic benefits\"   \n",
       "1                        \"Finance lease\"   \n",
       "2   \"Household distributed photovoltaic\"   \n",
       "3                           \"LV networks   \n",
       "4                    \"Photovoltaic loan\"   \n",
       "5                    % free PSA (% FPSA)   \n",
       "6              (EPV,Qd) operating points   \n",
       "7                (PV) Photovoltaic panel   \n",
       "8                    (V-P) voltage power   \n",
       "9             -Artificial-Neural-Network   \n",
       "10           -Back-propagation-algorithm   \n",
       "11                -DC/DC-boost-converter   \n",
       "12               -Fuzzy-Logic-Controller   \n",
       "13                 -Hybrid-Neurone-Fuzzy   \n",
       "14          -Maximum-Power-Point-Tracker   \n",
       "\n",
       "                                             1_unique  \\\n",
       "0                                  [economic benefit]   \n",
       "1                                     [finance lease]   \n",
       "2                [household distributed photovoltaic]   \n",
       "3                                        [lv network]   \n",
       "4                                 [photovoltaic loan]   \n",
       "5                                        [% free psa]   \n",
       "6                                   [operating point]   \n",
       "7                                [photovoltaic panel]   \n",
       "8                                     [voltage power]   \n",
       "9                         [artificial neural network]   \n",
       "10  [artificial neural network, artificial neural ...   \n",
       "11                            [dc/dc-boost-converter]   \n",
       "12                           [fuzzy-logic-controller]   \n",
       "13                             [hybrid-neurone-fuzzy]   \n",
       "14  [maximum-power-point-tracker, maximum-power-po...   \n",
       "\n",
       "                                    2_freq1000  \\\n",
       "0                                   [economic]   \n",
       "1                                           []   \n",
       "2       [photovoltaic, household, distributed]   \n",
       "3                                    [network]   \n",
       "4                               [photovoltaic]   \n",
       "5                                       [free]   \n",
       "6                           [operating, point]   \n",
       "7                        [photovoltaic, panel]   \n",
       "8                             [voltage, power]   \n",
       "9                                           []   \n",
       "10                                          []   \n",
       "11                      [converter, boost, dc]   \n",
       "12                         [controller, fuzzy]   \n",
       "13                             [hybrid, fuzzy]   \n",
       "14  [maximum, point, tracking, tracker, power]   \n",
       "\n",
       "                                          3_knowledge  \n",
       "0                                                  []  \n",
       "1                                                  []  \n",
       "2                                                  []  \n",
       "3                                                  []  \n",
       "4                                                  []  \n",
       "5                                                  []  \n",
       "6                                                  []  \n",
       "7                                                  []  \n",
       "8                                                  []  \n",
       "9   [neural network, machine learning, classificat...  \n",
       "10  [method, neural network, machine learning, alg...  \n",
       "11                                                 []  \n",
       "12                                                 []  \n",
       "13                                                 []  \n",
       "14                                                 []  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_values_refine = []\n",
    "for i, (freq_value, kn_value) in enumerate(zip(dict_freq_values, dict_kn_values)):\n",
    "    value_rm = []\n",
    "    for kn in kn_value:\n",
    "        if len(set(kn.split(\" \")) - set(freq_value)) == 0:\n",
    "            value_rm.extend(kn.split(\" \"))\n",
    "        if len(set(kn.split(\"-\")) - set(freq_value)) == 0:\n",
    "            value_rm.extend(kn.split(\" \"))\n",
    "    \n",
    "    freq_values_refine.append(list(set(freq_value)-set(value_rm)))\n",
    "\n",
    "df_dict_sum = pd.DataFrame({\"0_keywords\": dict_keys,\n",
    "                            \"1_unique\": dict_values,\n",
    "                            \"2_freq1000\": freq_values_refine,\n",
    "                            \"3_knowledge\": dict_kn_values\n",
    "                           })\n",
    "df_dict_sum.to_csv(\"keywords_summary.csv\", index=False)\n",
    "df_dict_sum.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. create summation of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_keywords</th>\n",
       "      <th>1_unique</th>\n",
       "      <th>2_freq1000</th>\n",
       "      <th>3_knowledge</th>\n",
       "      <th>2_or_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Economic benefits\"</td>\n",
       "      <td>[economic benefit]</td>\n",
       "      <td>[economic]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[economic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Finance lease\"</td>\n",
       "      <td>[finance lease]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Household distributed photovoltaic\"</td>\n",
       "      <td>[household distributed photovoltaic]</td>\n",
       "      <td>[photovoltaic, household, distributed]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[photovoltaic, household, distributed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"LV networks</td>\n",
       "      <td>[lv network]</td>\n",
       "      <td>[network]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[network]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Photovoltaic loan\"</td>\n",
       "      <td>[photovoltaic loan]</td>\n",
       "      <td>[photovoltaic]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[photovoltaic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>% free PSA (% FPSA)</td>\n",
       "      <td>[% free psa]</td>\n",
       "      <td>[free]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[free]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(EPV,Qd) operating points</td>\n",
       "      <td>[operating point]</td>\n",
       "      <td>[operating, point]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[operating, point]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(PV) Photovoltaic panel</td>\n",
       "      <td>[photovoltaic panel]</td>\n",
       "      <td>[photovoltaic, panel]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[photovoltaic, panel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(V-P) voltage power</td>\n",
       "      <td>[voltage power]</td>\n",
       "      <td>[voltage, power]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[voltage, power]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-Artificial-Neural-Network</td>\n",
       "      <td>[artificial neural network]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[neural network, machine learning, classificat...</td>\n",
       "      <td>[neural network, machine learning, classificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-Back-propagation-algorithm</td>\n",
       "      <td>[artificial neural network, artificial neural ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[method, neural network, machine learning, alg...</td>\n",
       "      <td>[method, neural network, machine learning, alg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-DC/DC-boost-converter</td>\n",
       "      <td>[dc/dc-boost-converter]</td>\n",
       "      <td>[converter, boost, dc]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[converter, boost, dc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-Fuzzy-Logic-Controller</td>\n",
       "      <td>[fuzzy-logic-controller]</td>\n",
       "      <td>[controller, fuzzy]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[controller, fuzzy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-Hybrid-Neurone-Fuzzy</td>\n",
       "      <td>[hybrid-neurone-fuzzy]</td>\n",
       "      <td>[hybrid, fuzzy]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[hybrid, fuzzy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-Maximum-Power-Point-Tracker</td>\n",
       "      <td>[maximum-power-point-tracker, maximum-power-po...</td>\n",
       "      <td>[maximum, point, tracking, tracker, power]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[maximum, point, tracking, tracker, power]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-Photovoltaic (PV)</td>\n",
       "      <td>[photovoltaic]</td>\n",
       "      <td>[photovoltaic]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[photovoltaic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1-D modeling</td>\n",
       "      <td>[1D modeling]</td>\n",
       "      <td>[modeling]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[modeling]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1-MHz frequency</td>\n",
       "      <td>[1-mhz frequency]</td>\n",
       "      <td>[frequency]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[frequency]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10-year predictions</td>\n",
       "      <td>[10-year prediction]</td>\n",
       "      <td>[prediction]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[prediction]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100% reliability</td>\n",
       "      <td>[100% reliability]</td>\n",
       "      <td>[reliability]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[reliability]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0_keywords  \\\n",
       "0                    \"Economic benefits\"   \n",
       "1                        \"Finance lease\"   \n",
       "2   \"Household distributed photovoltaic\"   \n",
       "3                           \"LV networks   \n",
       "4                    \"Photovoltaic loan\"   \n",
       "5                    % free PSA (% FPSA)   \n",
       "6              (EPV,Qd) operating points   \n",
       "7                (PV) Photovoltaic panel   \n",
       "8                    (V-P) voltage power   \n",
       "9             -Artificial-Neural-Network   \n",
       "10           -Back-propagation-algorithm   \n",
       "11                -DC/DC-boost-converter   \n",
       "12               -Fuzzy-Logic-Controller   \n",
       "13                 -Hybrid-Neurone-Fuzzy   \n",
       "14          -Maximum-Power-Point-Tracker   \n",
       "15                    -Photovoltaic (PV)   \n",
       "16                          1-D modeling   \n",
       "17                       1-MHz frequency   \n",
       "18                   10-year predictions   \n",
       "19                      100% reliability   \n",
       "\n",
       "                                             1_unique  \\\n",
       "0                                  [economic benefit]   \n",
       "1                                     [finance lease]   \n",
       "2                [household distributed photovoltaic]   \n",
       "3                                        [lv network]   \n",
       "4                                 [photovoltaic loan]   \n",
       "5                                        [% free psa]   \n",
       "6                                   [operating point]   \n",
       "7                                [photovoltaic panel]   \n",
       "8                                     [voltage power]   \n",
       "9                         [artificial neural network]   \n",
       "10  [artificial neural network, artificial neural ...   \n",
       "11                            [dc/dc-boost-converter]   \n",
       "12                           [fuzzy-logic-controller]   \n",
       "13                             [hybrid-neurone-fuzzy]   \n",
       "14  [maximum-power-point-tracker, maximum-power-po...   \n",
       "15                                     [photovoltaic]   \n",
       "16                                      [1D modeling]   \n",
       "17                                  [1-mhz frequency]   \n",
       "18                               [10-year prediction]   \n",
       "19                                 [100% reliability]   \n",
       "\n",
       "                                    2_freq1000  \\\n",
       "0                                   [economic]   \n",
       "1                                           []   \n",
       "2       [photovoltaic, household, distributed]   \n",
       "3                                    [network]   \n",
       "4                               [photovoltaic]   \n",
       "5                                       [free]   \n",
       "6                           [operating, point]   \n",
       "7                        [photovoltaic, panel]   \n",
       "8                             [voltage, power]   \n",
       "9                                           []   \n",
       "10                                          []   \n",
       "11                      [converter, boost, dc]   \n",
       "12                         [controller, fuzzy]   \n",
       "13                             [hybrid, fuzzy]   \n",
       "14  [maximum, point, tracking, tracker, power]   \n",
       "15                              [photovoltaic]   \n",
       "16                                  [modeling]   \n",
       "17                                 [frequency]   \n",
       "18                                [prediction]   \n",
       "19                               [reliability]   \n",
       "\n",
       "                                          3_knowledge  \\\n",
       "0                                                  []   \n",
       "1                                                  []   \n",
       "2                                                  []   \n",
       "3                                                  []   \n",
       "4                                                  []   \n",
       "5                                                  []   \n",
       "6                                                  []   \n",
       "7                                                  []   \n",
       "8                                                  []   \n",
       "9   [neural network, machine learning, classificat...   \n",
       "10  [method, neural network, machine learning, alg...   \n",
       "11                                                 []   \n",
       "12                                                 []   \n",
       "13                                                 []   \n",
       "14                                                 []   \n",
       "15                                                 []   \n",
       "16                                                 []   \n",
       "17                                                 []   \n",
       "18                                                 []   \n",
       "19                                                 []   \n",
       "\n",
       "                                               2_or_3  \n",
       "0                                          [economic]  \n",
       "1                                                  []  \n",
       "2              [photovoltaic, household, distributed]  \n",
       "3                                           [network]  \n",
       "4                                      [photovoltaic]  \n",
       "5                                              [free]  \n",
       "6                                  [operating, point]  \n",
       "7                               [photovoltaic, panel]  \n",
       "8                                    [voltage, power]  \n",
       "9   [neural network, machine learning, classificat...  \n",
       "10  [method, neural network, machine learning, alg...  \n",
       "11                             [converter, boost, dc]  \n",
       "12                                [controller, fuzzy]  \n",
       "13                                    [hybrid, fuzzy]  \n",
       "14         [maximum, point, tracking, tracker, power]  \n",
       "15                                     [photovoltaic]  \n",
       "16                                         [modeling]  \n",
       "17                                        [frequency]  \n",
       "18                                       [prediction]  \n",
       "19                                      [reliability]  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict_sum[\"2_or_3\"] = df_dict_sum[\"2_freq1000\"] + df_dict_sum[\"3_knowledge\"]\n",
    "df_dict_sum[\"2_or_3\"].apply(np.unique)\n",
    "\n",
    "# df_dict_sum[\"1_or_2_or_3\"] = df_dict_sum[\"1_unique\"] + df_dict_sum[\"2_or_3\"]\n",
    "# df_dict_sum[\"1_or_2_or_3\"].apply(np.unique)\n",
    "df_dict_sum.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. save as json file\n",
    "* save as `keywords_dict_final.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords_dict_final = dict(zip(dict_keys, df_dict_sum[\"1_or_2_or_3\"].values))\n",
    "keywords_dict_final = dict(zip(dict_keys, df_dict_sum[\"2_or_3\"].values))\n",
    "with open(\"keywords_dict_final.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_final, j, ensure_ascii=False, indent=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create WOS-format file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "WOSfilename = \"BIPV_ML_all.txt\"\n",
    "outfilename = \"BIPV_ML_nltk.txt\"\n",
    "kwfilename = \"keywords_dict_final.json\"\n",
    "\n",
    "manualfilename = \"keywords_manual.json\"\n",
    "with open(\"keywords_manual.json\", \"r\") as j:\n",
    "    keywords_manual = json.load(j)   \n",
    "manual_keys = list(keywords_manual.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 combine with other data\n",
    "### 5.2. extract keywords from abstract, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "# keywords_out_dict = {}\n",
    "\n",
    "with open(WOSfilename, \"r\") as infile:\n",
    "    with open(outfilename, \"w\") as outfile:\n",
    "        line = \" \"\n",
    "        \n",
    "        while line != \"EF\":\n",
    "            line = infile.readline()\n",
    "            if line[:3] == \"DE \":\n",
    "                line_DE = line\n",
    "                line_ID = infile.readline()\n",
    "                line_AB = infile.readline()\n",
    "                \n",
    "                count += 1\n",
    "            \n",
    "                # 1. Read from dictionary (keywords_dict_final.json)\n",
    "                kws_in = line_DE[3:].rstrip(\"\\n\").split(\"; \")\n",
    "                kws_out = []\n",
    "                for kw_in in kws_in:\n",
    "                    try:\n",
    "                        keyword = keywords_dict_final[kw_in]\n",
    "                        keyword = [re.sub(r'\\([^)]*\\)', \"\", keyword_).rstrip(' ').lstrip(' ').lower() for keyword_ in keyword]\n",
    "                        kws_out += [k for k in keyword if len(k)>1]\n",
    "                    except:\n",
    "                        pass\n",
    "#                         print(count, kw_in)\n",
    "                \n",
    "                # 2. Add keywords in relative dictionary\n",
    "                kws_out_ = deepcopy(kws_out)\n",
    "                for kw_out in kws_out:\n",
    "                    for manual_key in manual_keys:\n",
    "                        if manual_key.lower() in kw_out:\n",
    "                            manual_value = keywords_manual[manual_key]\n",
    "                            # from knowledge\n",
    "                            kn_values = dfs(keywords_knowledge_init, manual_value)\n",
    "                            kws_out_ += [k for k in kn_values if len(k)>1]\n",
    "                \n",
    "                # 3. Keyword mining from abstract\n",
    "                for manual_key in manual_keys:\n",
    "                    if manual_key.lower() in line_AB:\n",
    "                        manual_value = keywords_manual[manual_key]\n",
    "                        # from knowledge\n",
    "                        kn_values = dfs(keywords_knowledge_init, manual_value)\n",
    "                        kws_out_ += [k for k in kn_values if len(k)>1]\n",
    "                \n",
    "                kws_out_ = np.unique(kws_out_)\n",
    "                kws_out = deepcopy(kws_out_)\n",
    "                for kw_out in kws_out_:\n",
    "                    for manual_key in manual_keys:\n",
    "                        if manual_key.lower() in kw_out:\n",
    "                            kws_out = [k for k in list(set(kws_out) | set(keywords_manual[manual_key])) if len(k)>1]\n",
    "                \n",
    "                # 4. cleaning bad keywords\n",
    "                if 'none' in kws_out:\n",
    "                    list(kws_out).remove(\"none\")                \n",
    "                \n",
    "                # 5. write on outfile\n",
    "                DE_out = \"DE \" + \"; \".join(kws_out) + \"\\n\"\n",
    "                outfile.write(DE_out)\n",
    "                outfile.write(line_ID)\n",
    "                outfile.write(line_AB)\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "                \n",
    "# with open(\"keywords_nltk_out.json\", \"w\") as j:\n",
    "#     json.dump(keywords_out, j, sort_keys=True, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
