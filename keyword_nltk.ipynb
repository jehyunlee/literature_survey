{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re, nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "Lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus = pd.read_csv(\"scopus.csv\")\n",
    "keyword_org_name = \"keywords.txt\"\n",
    "keyword_abb_name = \"keyword_abb.json\"\n",
    "keyword_dash_name = \"keywords_dash.txt\"\n",
    "keyword_dict_name = \"keywords_dict.json\"\n",
    "keyword_single_name = \"keywords_single.txt\"\n",
    "keyword_plural_name = \"keywords_plural.json\"\n",
    "unicode_name = \"unicode_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unicode dictionary\n",
    "with open('unicode_dict.json') as j:\n",
    "    unicode_dict = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "line = ''\n",
    "\n",
    "with open(\"BIPV_ML_all.txt\") as f:\n",
    "    while line != \"EF\":\n",
    "        line = f.readline()\n",
    "        if line[:3] == \"DE \":\n",
    "            keywords_new = line[3:].rstrip('\\n').split(\";\")\n",
    "            for keyword_new in keywords_new:\n",
    "                keywords += [keyword_new.rstrip(' ').lstrip(' ')]\n",
    "\n",
    "keywords = sorted(keywords)\n",
    "with open(keyword_org_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords:\n",
    "        f.write(f\"{keyword}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive keywords\n",
    "keywords_raw_ = np.genfromtxt(\"keywords.txt\", dtype=\"str\", delimiter=\"\\n\", encoding='utf-8-sig') #utf-8-sig for \\ufeff removal\n",
    "\n",
    "# get unique ones\n",
    "keywords_raw_ = np.unique(keywords_raw_)\n",
    "\n",
    "# abbriviations detection (key candidates)\n",
    "keywords_abb = {}\n",
    "keywords_abb_key_cand = []\n",
    "for keyword_raw_ in keywords_raw_:\n",
    "    kwr = keyword_raw_.split()\n",
    "    for kw in kwr:\n",
    "        kw = kw.lstrip(\"(\").rstrip(\")\")\n",
    "        if (kw == kw.upper()) \\\n",
    "           and (kw.lstrip(\"(\").rstrip(\")\") not in keywords_abb_key_cand) \\\n",
    "           and (ord(kw[0]) >= ord('A') and ord(kw[0]) <= ord('Z')) \\\n",
    "           and (len(kw) > 1):\n",
    "            keywords_abb_key_cand.append(kw.lstrip(\"(\").rstrip(\")\").rstrip(\":\").rstrip(\",\"))\n",
    "\n",
    "# convert to lower cases\n",
    "keywords_raw = [kw.lower() for kw in keywords_raw_]\n",
    "\n",
    "# duplicate for values\n",
    "keywords_value = deepcopy(keywords_raw)\n",
    "\n",
    "### keywords cleaning\n",
    "keywords_dash = [] # additional dictionary, for words containing '-'\n",
    "\n",
    "# remove staring characters\n",
    "keywords_value = [kw.lstrip('-') for kw in keywords_value]\n",
    "keywords_value = [kw.rstrip('-') for kw in keywords_value]\n",
    "keywords_value = [kw.lstrip(':') for kw in keywords_value]\n",
    "keywords_value = [kw.rstrip(':') for kw in keywords_value]\n",
    "\n",
    "# remove brakets \"()\"\n",
    "for kc in keywords_abb_key_cand:\n",
    "    kc_ = kc.lower()\n",
    "    for kw in keywords_value:\n",
    "        if kc_ in kw.replace('(', \"\").replace(')', \"\").split():\n",
    "            kc_value = re.sub(r'\\([^)]*\\)', \"\", kw).rstrip(' ').lstrip(' ')\n",
    "            if (kc_ not in kc_value) and \\\n",
    "               (kc_value[-1] != \")\") and \\\n",
    "               (len(kc_value.split()) > 1) and \\\n",
    "               ((kc not in list(keywords_abb.keys())) or (kc in list(keywords_abb.keys()) and len(kc_value) < len(keywords_abb[kc]) and len(kc_value.split()) > 1)):\n",
    "                kc_value = kc_value.replace(\"  \", \" \")\n",
    "                kc_value = ' '.join(kc_value.split(' ')[:-1] + [Lem.lemmatize(kc_value.split(' ')[-1])])\n",
    "                keywords_abb.update({kc:kc_value})\n",
    "\n",
    "keywords_value = [re.sub(r'\\([^)]*\\)', \"\", kw).rstrip(' ').lstrip(' ').lstrip(')').lstrip('(') for kw in keywords_value]\n",
    "\n",
    "# remove unicodes â€“\n",
    "unicode_keys = list(unicode_dict.keys())\n",
    "for ukey in unicode_keys:\n",
    "        keywords_value = [kw.replace(ukey, unicode_dict[ukey]) for kw in keywords_value]\n",
    "\n",
    "# remove mathematics\n",
    "keywords_value = [kw.replace(\"\\\\infty\", \"infinity\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"\\\\mathrm{\", \"\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"{\", \"\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"}\", \"\") for kw in keywords_value]\n",
    "\n",
    "# remove '\"'\n",
    "keywords_value = [kw.replace('\"', \"\") for kw in keywords_value]\n",
    "\n",
    "# reduce needless blanks.\n",
    "keywords_value = [re.sub(\"\\s+\", \" \", kw) for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\" -\", \"-\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"- \", \"-\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\" //\", \"//\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"// \", \"//\") for kw in keywords_value]\n",
    "\n",
    "# create single-word keyword\n",
    "keywords_single = []\n",
    "for kw in keywords_value:\n",
    "    if len(kw.split()) == 1 and len(kw) <= 4:\n",
    "        keywords_single.append(kw)\n",
    "\n",
    "# Convert plural to singular\n",
    "Lem = WordNetLemmatizer()\n",
    "keywords_plural = {}\n",
    "keywords_value_ = []\n",
    "for i, kw in enumerate(keywords_value):\n",
    "    if (kw not in keywords_single) and (len(kw.split(' ')) > 1):\n",
    "        kw_ = ' '.join(kw.split(' ')[:-1] + [Lem.lemmatize(kw.split(' ')[-1])])\n",
    "        keywords_value_.append(kw_)\n",
    "    else:\n",
    "        keywords_value_.append(kw)\n",
    "        \n",
    "    kw_ = kw.split(' ')\n",
    "    for kw__ in kw_:\n",
    "        if kw__ not in keywords_single and Lem.lemmatize(kw__) != kw__:\n",
    "            keywords_plural.update({kw__: Lem.lemmatize(kw__)})\n",
    "\n",
    "keywords_value = deepcopy(keywords_value_)\n",
    "        \n",
    "    \n",
    "# keywords_value = [' '.join(kw.split(' ')[:-1] + [Lem.lemmatize(kw.split(' ')[-1])] if len(kw.split(' ')) > 1 and not in keywords_single else kw for kw in keywords_value]\n",
    "\n",
    "keywords_value = [kw.lstrip(' ').rstrip(' ') for kw in keywords_value]\n",
    "    \n",
    "# create dash dictionary\n",
    "for kw in keywords_value:\n",
    "    kw_ = kw.split(\" \")\n",
    "    for kw__ in kw_:\n",
    "        if ('-' in kw__) or ('//' in kw__):\n",
    "            keywords_dash.append(kw__)\n",
    "    \n",
    "keywords_dash = np.unique(keywords_dash)\n",
    "\n",
    "# create dictionary\n",
    "keywords_dict = dict(zip(keywords_raw, keywords_value))\n",
    "with open(keyword_dict_name, \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)\n",
    "    \n",
    "# create plural-singular dictionary\n",
    "plural_sorted = dict(sorted(keywords_plural.items()))\n",
    "with open(keyword_plural_name, \"w\") as j:\n",
    "    json.dump(plural_sorted, j, ensure_ascii=False, indent=2)\n",
    "\n",
    "# create abbreviation dictionary\n",
    "abb_sorted = dict(sorted(keywords_abb.items()))\n",
    "with open(keyword_abb_name, \"w\") as j:\n",
    "    json.dump(abb_sorted, j, ensure_ascii=False, indent=2)\n",
    "\n",
    "# create \"dash\" keywords list\n",
    "with open(keyword_dash_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords_dash:\n",
    "        f.write(f\"{keyword}\\n\")\n",
    "\n",
    "# create \"dash\" keywords list\n",
    "with open(keyword_single_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords_single:\n",
    "        f.write(f\"{keyword}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* find words matching with \"keywords_dash\" with same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_value_sort = np.unique(keywords_value, return_counts=True)\n",
    "\n",
    "df_keywords_value = pd.DataFrame({\"keyword\": keywords_value_sort[0],\n",
    "                                         \"counts\": keywords_value_sort[1],\n",
    "                                        }).sort_values(\"keyword\")\n",
    "df_keywords_value.to_csv(\"keywords_value.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "substring not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-4254ac41000b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"def b ebbzf c\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtmp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-4254ac41000b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"def b ebbzf c\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtmp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: substring not found"
     ]
    }
   ],
   "source": [
    "A = [\"a\", \"b\", \"c\"]\n",
    "B = \"def b ebbzf c\"\n",
    "\n",
    "tmp = [B.index(a) for a in A]\n",
    "tmp1 = sorted(tmp)\n",
    "print(tmp, tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try01 ['36-cell pv module'] 6 p-v [1, 1]\n",
      "try1 ['36-cell pv module'] 6 pv-module [1, 2]\n",
      "try01 ['3d'] 7 3-d [1, 2]\n",
      "try02 ['3d'] 7 3-d [1, 2]\n",
      "try01 ['3d building model'] 8 3-d [1, 2]\n",
      "try01 ['3d city model'] 9 3-d [1, 2]\n",
      "try01 ['3d density model'] 10 3-d [1, 2]\n",
      "try01 ['3d experiment'] 11 3-d [1, 2]\n",
      "try01 ['3d finite element'] 12 3-d [1, 2]\n",
      "try1 ['3d finite element'] 12 finite-element [1, 2]\n",
      "try01 ['3d finite-volume modeling'] 13 3-d [1, 2]\n",
      "try01 ['3d gi'] 14 3-d [1, 2]\n",
      "try01 ['3d model'] 15 3-d [1, 2]\n",
      "try01 ['3d numerical model'] 16 3-d [1, 2]\n",
      "try01 ['3d radiative transfer'] 17 3-d [1, 2]\n",
      "try01 ['3d solar cell modeling'] 18 3-d [1, 2]\n",
      "try01 ['3d solar city'] 19 3-d [1, 2]\n",
      "try01 ['3d solar radiation model'] 20 3-d [1, 2]\n",
      "try01 ['3d urban model'] 21 3-d [1, 2]\n",
      "try02 ['3d-tlm'] 22 3-d [1, 2]\n",
      "try2 ['3d-tlm'] 22 3d-tlm [0, 1]\n",
      "try2 ['3l-anpc'] 23 3l-anpc [0, 1]\n",
      "try2 ['4-terminal'] 24 4-terminal [0, 1]\n",
      "try1 ['5-level single phase converter'] 27 single-phase [1, 2]\n"
     ]
    }
   ],
   "source": [
    "df_keywords_value = pd.read_csv(\"keywords_value.csv\")\n",
    "\n",
    "df_tmp = df_keywords_value.iloc[30:60].reset_index(drop=True)\n",
    "df_tmp[\"relwords\"] = np.nan\n",
    "\n",
    "for i in range(30):\n",
    "    words0 = df_tmp[\"keyword\"].iloc[i]\n",
    "    words1 = words0.split(\" \")\n",
    "    words2 = words0.split(\"-\")\n",
    "    \n",
    "    for kws in keywords_dash:\n",
    "        kws_ = kws.replace(\"-\", \"\")\n",
    "        if kws_ in words1:\n",
    "            print(\"try01\", [df_tmp[\"keyword\"].iloc[i]], i, kws, tmp)\n",
    "\n",
    "        \n",
    "        \n",
    "        kws_ = kws.split('-')\n",
    "        try:\n",
    "            tmp = [words1.index(kw_) for kw_ in kws_]\n",
    "            if tmp == sorted(tmp) and (len(np.unique(tmp)) == len(tmp)):\n",
    "                print(\"try1\", [df_tmp[\"keyword\"].iloc[i]], i, kws, tmp)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                tmp = [words2.index(kw_) for kw_ in kws_]\n",
    "                if tmp == sorted(tmp) and (len(np.unique(tmp)) == len(tmp)):\n",
    "                    print(\"try2\", [df_tmp[\"keyword\"].iloc[i]], i, kws, tmp)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_value_single = []\n",
    "p = re.compile(\"[a-zA-Z]\")\n",
    "\n",
    "abbkeys_ = list(keywords_abb.keys())\n",
    "abbkeys = [key.lower() for key in abbkeys]\n",
    "pluralkeys = list(keywords_plural.keys())\n",
    "\n",
    "for keyword_value in keywords_value:\n",
    "    kws = keyword_value.replace(',', \"\").split(' ')\n",
    "    for kw in kws:\n",
    "        if p.match(kw) and (kw not in abbkeys) and (kw not in keywords_dash):\n",
    "            keywords_value_single.append(kw)\n",
    "\n",
    "keywords_value_single = np.unique(keywords_value_single, return_counts=True)\n",
    "\n",
    "df_keywords_value_single = pd.DataFrame({\"keyword\": keywords_value_single[0],\n",
    "                                         \"counts\": keywords_value_single[1],\n",
    "                                         \"relwords\": [keywords_plural[kw] if kw in pluralkeys else kw for kw in keywords_value_single[0]]\n",
    "                                        })\n",
    "df_keywords_value_single[df_keywords_value_single[\"counts\"]>5].to_csv(\"keywords_relwords.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* manual operation on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_noun_dict = {}\n",
    "keywords_noun_ = list(keywords_abb.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
