{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re, nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "Lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus = pd.read_csv(\"scopus.csv\")\n",
    "keyword_org_name = \"keywords.txt\"\n",
    "keyword_abb_name = \"keywords_abb.json\"\n",
    "keyword_dash_name = \"keywords_dash.json\"\n",
    "keyword_dict_name = \"keywords_dict.json\"\n",
    "keyword_single_name = \"keywords_single.txt\"\n",
    "keyword_plural_name = \"keywords_plural.json\"\n",
    "unicode_name = \"unicode_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unicode dictionary\n",
    "with open('unicode_dict.json') as j:\n",
    "    unicode_dict = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "line = ''\n",
    "\n",
    "with open(\"BIPV_ML_all.txt\") as f:\n",
    "    while line != \"EF\":\n",
    "        line = f.readline()\n",
    "        if line[:3] == \"DE \":\n",
    "            keywords_new = line[3:].rstrip('\\n').split(\";\")\n",
    "            for keyword_new in keywords_new:\n",
    "                keywords += [keyword_new.rstrip(' ').lstrip(' ')]\n",
    "\n",
    "keywords = sorted(keywords)\n",
    "with open(keyword_org_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords:\n",
    "        f.write(f\"{keyword}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aerosols\n",
      "aggregators\n",
      "agrivoltaics\n",
      "algorithms\n",
      "barriers\n",
      "bifurcations\n",
      "buildings\n",
      "characteristics\n",
      "choppers\n",
      "clouds\n",
      "consumers\n",
      "controllers\n",
      "converters\n",
      "correlations\n",
      "costs\n",
      "delays\n",
      "dielectrics\n",
      "diodes\n",
      "dislocations\n",
      "dynamics\n",
      "economics\n",
      "eigenvalues\n",
      "electrolyzers\n",
      "emissions\n",
      "ensembles\n",
      "experiments\n",
      "failures\n",
      "faults\n",
      "feed-in-tariffs\n",
      "forecasts\n",
      "greenhouses\n",
      "harmonics\n",
      "heterojunctions\n",
      "households\n",
      "hurricanes\n",
      "ibscs\n",
      "igbts\n",
      "imports\n",
      "inverters\n",
      "irradiances\n",
      "measurements\n",
      "metaheuristics\n",
      "micro-grids\n",
      "microgrids\n",
      "microinverters\n",
      "microturbines\n",
      "mini-grids\n",
      "modes\n",
      "models\n",
      "modules\n",
      "multi-junctions\n",
      "nano-grids\n",
      "nanofluids\n",
      "nanowires\n",
      "optoelectronics\n",
      "performances\n",
      "perovskites\n",
      "photodetectors\n",
      "photodiodes\n",
      "photovoltaics\n",
      "prosumers\n",
      "relays\n",
      "renewables\n",
      "resonances\n",
      "scenarios\n",
      "semiconductors\n",
      "sensors\n",
      "simulations\n",
      "simulators\n",
      "supercapacitors\n",
      "systems\n",
      "techno-economics\n",
      "technoeconomics\n",
      "thermodynamics\n",
      "thermophotovoltaics\n",
      "transients\n",
      "trends\n",
      "voltages\n",
      "waves\n",
      "wavelets\n"
     ]
    }
   ],
   "source": [
    "# retreive keywords\n",
    "keywords_raw_ = np.genfromtxt(\"keywords.txt\", dtype=\"str\", delimiter=\"\\n\", encoding='utf-8-sig') #utf-8-sig for \\ufeff removal\n",
    "\n",
    "# get unique ones\n",
    "keywords_raw = np.unique(keywords_raw_)\n",
    "\n",
    "# abbriviations detection (key candidates)\n",
    "keywords_abb = {}\n",
    "keywords_abb_key_cand = []\n",
    "for keyword_raw_ in keywords_raw_:\n",
    "    kwr = keyword_raw_.split()\n",
    "    for kw in kwr:\n",
    "        kw = kw.lstrip(\"(\").rstrip(\")\")\n",
    "        if (kw == kw.upper()) \\\n",
    "           and (kw.lstrip(\"(\").rstrip(\")\") not in keywords_abb_key_cand) \\\n",
    "           and (ord(kw[0]) >= ord('A') and ord(kw[0]) <= ord('Z')) \\\n",
    "           and (len(kw) > 1):\n",
    "            keywords_abb_key_cand.append(kw.lstrip(\"(\").rstrip(\")\").rstrip(\":\").rstrip(\",\"))\n",
    "\n",
    "# convert to lower cases\n",
    "# keywords_raw = [kw.lower() for kw in keywords_raw_]\n",
    "\n",
    "# duplicate for values, and convert to lower cases\n",
    "keywords_value = deepcopy(keywords_raw)\n",
    "\n",
    "# convert to lower cases\n",
    "keywords_value = [kw.lower() for kw in keywords_value]\n",
    "\n",
    "# remove staring characters\n",
    "keywords_value = [kw.lstrip('-') for kw in keywords_value]\n",
    "keywords_value = [kw.rstrip('-') for kw in keywords_value]\n",
    "keywords_value = [kw.lstrip(':') for kw in keywords_value]\n",
    "keywords_value = [kw.rstrip(':') for kw in keywords_value]\n",
    "\n",
    "# remove brakets \"()\"\n",
    "for kc in keywords_abb_key_cand:\n",
    "    kc_ = kc.lower()\n",
    "    for kw in keywords_value:\n",
    "        if kc_ in kw.replace('(', \"\").replace(')', \"\").split():\n",
    "            kc_value = re.sub(r'\\([^)]*\\)', \"\", kw).rstrip(' ').lstrip(' ')\n",
    "            if (kc_ not in kc_value) and \\\n",
    "               (kc_value[-1] != \")\") and \\\n",
    "               (len(kc_value.split()) > 1) and \\\n",
    "               ((kc not in list(keywords_abb.keys())) or (kc in list(keywords_abb.keys()) and len(kc_value) < len(keywords_abb[kc]) and len(kc_value.split()) > 1)):\n",
    "                kc_value = kc_value.replace(\"  \", \" \")\n",
    "                kc_value = ' '.join(kc_value.split(' ')[:-1] + [Lem.lemmatize(kc_value.split(' ')[-1])])\n",
    "                keywords_abb.update({kc:kc_value})\n",
    "\n",
    "keywords_value = [re.sub(r'\\([^)]*\\)', \"\", kw).rstrip(' ').lstrip(' ').lstrip(')').lstrip('(') for kw in keywords_value]\n",
    "\n",
    "# remove unicodes â€“\n",
    "unicode_keys = list(unicode_dict.keys())\n",
    "for ukey in unicode_keys:\n",
    "        keywords_value = [kw.replace(ukey, unicode_dict[ukey]) for kw in keywords_value]\n",
    "\n",
    "# remove mathematics\n",
    "keywords_value = [kw.replace(\"\\\\infty\", \"infinity\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"\\\\mathrm{\", \"\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"{\", \"\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"}\", \"\") for kw in keywords_value]\n",
    "\n",
    "# remove '\"'\n",
    "keywords_value = [kw.replace('\"', \"\") for kw in keywords_value]\n",
    "\n",
    "# reduce needless blanks.\n",
    "keywords_value = [re.sub(\"\\s+\", \" \", kw) for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\" -\", \"-\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"- \", \"-\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\" //\", \"//\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"// \", \"//\") for kw in keywords_value]\n",
    "\n",
    "# create single-word keyword\n",
    "keywords_single = [k for k in list(keywords_abb.keys())]\n",
    "for kws in keywords_value:\n",
    "    if len(kws.split(\" \")) == 1 and len(kw) <= 4:\n",
    "        keywords_single.append(kws)\n",
    "    for kw in kws:\n",
    "        if len(kw.split(\"-\")) == 1 and len(kw) <= 4 and len(kw) > 1:\n",
    "            keywords_single.append(kw)\n",
    "\n",
    "keywords_single = list(np.unique(sorted(keywords_single)))\n",
    "\n",
    "keywords_single_ = deepcopy(keywords_single)       \n",
    "for kw in keywords_single_:\n",
    "    if kw+\"s\" in keywords_single and len(kw) >= 4:\n",
    "        print(kw+\"s\")\n",
    "        keywords_single.remove(kw+\"s\")\n",
    "    if kw[-1]+\"ies\" in keywords_single and kw[-1] == \"y\" and len(kw >=4):\n",
    "        print(kw+\"s\")\n",
    "        keywords_single.remove(kw+\"s\")\n",
    "\n",
    "# create abbreviation dictionary\n",
    "abb_sorted = dict(sorted(keywords_abb.items()))\n",
    "with open(keyword_abb_name, \"w\") as j:\n",
    "    json.dump(abb_sorted, j, ensure_ascii=False, indent=2)\n",
    "    \n",
    "# create \"single\" keywords list\n",
    "with open(keyword_single_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords_single:\n",
    "        f.write(f\"{keyword}\\n\")\n",
    "\n",
    "# create \"value\"\n",
    "keywords_dict = dict(zip(keywords_raw, keywords_value))\n",
    "with open(keyword_dict_name, \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual refine `keywords_single`\n",
    "* plural words not to be kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve keywords_value\n",
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)\n",
    "keywords_value = list(keywords_dict.values())\n",
    "keywords_key = list(keywords_dict.keys())\n",
    "\n",
    "# retreive keywords_single\n",
    "keywords_single = np.genfromtxt(\"keywords_single_refine.txt\", dtype=\"str\", delimiter=\"\\n\", encoding='utf-8-sig') #utf-8-sig for \\ufeff removal\n",
    "keywords_single = [k.lower() for k in keywords_single]\n",
    "\n",
    "# Convert plural to singular\n",
    "Lem = WordNetLemmatizer()\n",
    "keywords_plural = {}\n",
    "keywords_value_ = []\n",
    "for kws in keywords_value:\n",
    "    if (kws not in keywords_single) and (len(kws.split(\" \")) > 1):\n",
    "        kws_0 = kws.split(\" \")\n",
    "        \n",
    "        for kws_1 in kws_0:\n",
    "            kws_2 = kws_1.split(\"-\")\n",
    "            for kw in kws_2:\n",
    "                if (kw not in keywords_single) and (kw != Lem.lemmatize(kw)):\n",
    "                    kws = kws.replace(kw, Lem.lemmatize(kw))\n",
    "                    keywords_plural.update({kw: Lem.lemmatize(kw)})\n",
    "        keywords_value_.append(kws)\n",
    "    else:\n",
    "        keywords_value_.append(kws)\n",
    "\n",
    "keywords_value = deepcopy(keywords_value_)\n",
    "keywords_value = [kw.lstrip(' ').rstrip(' ') for kw in keywords_value]\n",
    "\n",
    "    \n",
    "# create plural-singular dictionary\n",
    "plural_sorted = dict(sorted(keywords_plural.items()))\n",
    "with open(keyword_plural_name, \"w\") as j:\n",
    "    json.dump(plural_sorted, j, ensure_ascii=False, indent=2)\n",
    "    \n",
    "# create \"value\"\n",
    "keywords_dict = dict(zip(keywords_key, keywords_value))\n",
    "with open(keyword_dict_name, \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve keywords_value\n",
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)\n",
    "keywords_value = list(keywords_dict.values())\n",
    "\n",
    "# create dash dictionary\n",
    "keywords_dash = []\n",
    "for kw in keywords_value:\n",
    "    kw_ = kw.split(\" \")\n",
    "    for kw__ in kw_:\n",
    "        if ('-' in kw__) or ('//' in kw__):\n",
    "            keywords_dash.append(kw__)\n",
    "    \n",
    "keywords_dash = np.unique(keywords_dash)\n",
    "\n",
    "# create \"dash\" keywords dictionary - manual refinement required\n",
    "keywords_dash_dict = dict(zip(keywords_dash, keywords_dash))\n",
    "with open(keyword_dash_name, \"w\") as j:\n",
    "    json.dump(keywords_dash_dict, j, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual keywords-abb refinement\n",
    "* incorrect names, typo errors\n",
    "* unifying same words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual keyworkds-dash refinement\n",
    "* incorrect names, typo errors\n",
    "* unifying same words (ex. photo-voltaic and photovoltaic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_plural.json\", \"r\") as j:\n",
    "    keywords_plural = json.load(j)\n",
    "\n",
    "with open(\"keywords_abb_refine.json\", \"r\") as j:\n",
    "    keywords_abb_refine = json.load(j)\n",
    "\n",
    "with open(\"keywords_dash_refine.json\", \"r\") as j:\n",
    "    keywords_dash_refine = json.load(j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "abb_refine_keys = list(keywords_abb_refine.keys())\n",
    "dash_refine_keys = list(keywords_dash_refine.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dash_refine2 = {}\n",
    "\n",
    "plural_keys = list(keywords_plural.keys())\n",
    "\n",
    "for dkey in dash_refine_keys:\n",
    "    dvalues = keywords_dash_refine[dkey].split(\"; \")\n",
    "    \n",
    "    dvalues_abb = []\n",
    "    dvalues_rms = []\n",
    "    for dvalue in dvalues:\n",
    "        dvalue_rem = deepcopy(dvalue)\n",
    "        for akey in abb_refine_keys:\n",
    "            if dvalue == akey.lower():\n",
    "                dvalues.remove(dvalue)\n",
    "                dvalues.append(keywords_abb_refine[akey])\n",
    "            else:\n",
    "                [dvalues_abb.append(keywords_abb_refine[d.upper()]) for d in dvalue.split(\"-\") if d == akey.lower()]\n",
    "                [dvalues_abb.append(keywords_abb_refine[d.upper()]) for d in dvalue.split(\"/\") if d == akey.lower()]\n",
    "\n",
    "    keywords_dash_refine2.update({dkey:dvalues+dvalues_abb})\n",
    "\n",
    "with open(\"keywords_dash_refine2.json\", \"w\") as j:\n",
    "    json.dump(keywords_dash_refine2, j, ensure_ascii=False, indent=2)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual keyworkds_dash2 refinement\n",
    "* incorrect abbs treatement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply keywords-dash on keywords_value\n",
    "* (1) replace keywords_dash_refine.keys() to keywords_dash_refine.values()\n",
    "* (2) if keywords_dash_refine2.key() has more than 2 elements, add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_dash_refine2.json\", \"r\") as j:\n",
    "    keywords_dash_refine2 = json.load(j)\n",
    "    \n",
    "\n",
    "keywords_value_ = []\n",
    "for kws in keywords_value:\n",
    "    dash_flag = 0\n",
    "    kws_ = []\n",
    "    for dkey in dash_refine_keys:\n",
    "        \n",
    "        if dkey in kws:\n",
    "            kws_ += [kws.replace(dkey, keywords_dash_refine[dkey])]\n",
    "            dash_flag += 1\n",
    "            \n",
    "            if len(keywords_dash_refine2[dkey]) > 1:\n",
    "                [kws_.append(v) for v in keywords_dash_refine2[dkey][1:]]\n",
    "                \n",
    "    if dash_flag == 0:\n",
    "        kws_ = [kws]\n",
    "    \n",
    "    keywords_value_.append(list(np.unique(kws_)))\n",
    "\n",
    "keywords_dict = dict(zip(list(keywords_dict.keys()), keywords_value_))\n",
    "with open(keyword_dict_name, \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find and extract dash and abbriviations\n",
    "* create `keywords_dict_add.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 16s, sys: 160 ms, total: 4min 17s\n",
      "Wall time: 4min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "abb_refine_keys = [k.lower() for k in np.unique(list(keywords_abb_refine.keys()))]\n",
    "abb_refine_values = [v.lower() for v in np.unique(list(keywords_abb_refine.values()))]\n",
    "\n",
    "keywords_value_add = []\n",
    "\n",
    "for kw in keywords_value:\n",
    "    kw_0 = [kw]\n",
    "    kw_1 = kw.split(\" \")\n",
    "    kw_2 = kw.split(\"-\")\n",
    "    kw_3 = kw.split(\"/\")\n",
    "    \n",
    "    keywords_value_add_ = []\n",
    "    \n",
    "    # \"dash\"\n",
    "    for dash_keys in dash_refine_keys:\n",
    "        if dash_keys in kw_1:\n",
    "            [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "        \n",
    "        dash_keys_ = dash_keys.replace(\"-\", \"\")\n",
    "        if dash_keys_ in kw_1:\n",
    "            if not (dash_keys_ in abb_refine_keys): # original abbriviation should be kept\n",
    "                [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "\n",
    "        dash_keys_ = dash_keys.split(\"-\")\n",
    "        if not (dash_keys_ in abb_refine_keys): # original abbriviation should be kept\n",
    "            try:\n",
    "                tmp = [kw_1.index(dash_key_) for dash_key_ in dash_keys_]\n",
    "                if tmp == sorted(tmp) and (len(np.unique(tmp)) == len(tmp)):\n",
    "                    [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    tmp = [kw_2.index(dash_key_) for dash_key_ in dash_keys_]\n",
    "                    if tmp == sorted(tmp) and (len(np.unique(tmp)) == len(tmp)):\n",
    "                        [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    \n",
    "    # \"abb\"\n",
    "    for abb_vals in abb_refine_values:\n",
    "        if abb_vals in kw and \\\n",
    "           (abb_vals.replace(\" \", \"-\") not in keywords_value_add) and \\\n",
    "           (abb_vals.replace(\"-\", \" \") not in keywords_value_add):\n",
    "            keywords_value_add_.append(abb_vals)\n",
    "    \n",
    "    for abb_keys in abb_refine_keys:\n",
    "        if abb_keys in (kw_0 + kw_1 + kw_2 + kw_3):\n",
    "            keywords_value_add_.append(keywords_abb_refine[abb_keys.upper()])\n",
    "    \n",
    "    keywords_value_add.append(list(np.unique(keywords_value_add_)))\n",
    "\n",
    "# create \"additional\" keywords dictionary\n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, keywords_value_add))\n",
    "with open(\"keywords_dict_add_dashabb.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update `keywords_manual` on `keywords_dict` and `keywords_dict_add_dashabb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_manual.json\", \"r\") as j:\n",
    "    keywords_manual = json.load(j)\n",
    "    \n",
    "with open(\"keywords_dict_add_dashabb.json\", \"r\") as j:\n",
    "    keywords_dict_add_dashabb = json.load(j)\n",
    "    \n",
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_keys = list(keywords_manual.keys())\n",
    "dict_values = list(keywords_dict.values())\n",
    "add_dashabb_values = list(keywords_dict_add_dashabb.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values_update = []\n",
    "\n",
    "for values in dict_values:\n",
    "    value_new = []\n",
    "    for value in values:\n",
    "        for mkey in manual_keys:\n",
    "            mkey_ = mkey.lower()\n",
    "            value = value.replace(mkey_, keywords_manual[mkey])\n",
    "        value_new.append(value)\n",
    "    dict_values_update.append(value_new)\n",
    "    \n",
    "keywords_dict = dict(zip(keywords_raw, dict_values_update))\n",
    "with open(\"keywords_dict2.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dashabb_values_update = []\n",
    "\n",
    "for values in add_dashabb_values:\n",
    "    value_new = []\n",
    "    for value in values:\n",
    "        for mkey in manual_keys:\n",
    "            mkey_ = mkey.lower()\n",
    "            value = value.replace(mkey_, keywords_manual[mkey])\n",
    "        value_new.append(value)\n",
    "    add_dashabb_values_update.append(value_new)\n",
    "    \n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, add_dashabb_values_update))\n",
    "with open(\"keywords_dict_add_dashabb2.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove almost same words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10237</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5297</th>\n",
       "      <td>generation</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9100</th>\n",
       "      <td>neural network</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8131</th>\n",
       "      <td>micro-grid</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10434</th>\n",
       "      <td>photovoltaic system</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4232</th>\n",
       "      <td>energy storage</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8162</th>\n",
       "      <td>microgrid</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6645</th>\n",
       "      <td>infrared</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11627</th>\n",
       "      <td>pv-system</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4293</th>\n",
       "      <td>energy-storage</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keywords  counts\n",
       "10237         photovoltaic    1828\n",
       "5297            generation     257\n",
       "9100        neural network     178\n",
       "8131            micro-grid     151\n",
       "10434  photovoltaic system     120\n",
       "4232        energy storage     119\n",
       "8162             microgrid     115\n",
       "6645              infrared     115\n",
       "11627            pv-system     112\n",
       "4293        energy-storage     106"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "merged = list(itertools.chain.from_iterable([dict_values_update, add_dashabb_values_update]))\n",
    "merged = list(itertools.chain.from_iterable(merged))\n",
    "merged_u = np.unique(merged, return_counts=True)\n",
    "\n",
    "words_u = merged_u[0]\n",
    "   \n",
    "\n",
    "df_words = pd.DataFrame({\"keywords\": merged_u[0],\n",
    "                         \"counts\": merged_u[1]\n",
    "                        }).sort_values(\"counts\", ascending=False)\n",
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for word_u in words_u:\n",
    "#     if (word_u.replace(\"-\", \" \") in words_u) and (\"-\" in word_u):\n",
    "#         count += 1\n",
    "#         print(count, word_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dash_update = {}\n",
    "\n",
    "count = 0\n",
    "for word in words_u:\n",
    "    word_wo_dash = word.replace(\"-\", \"\")\n",
    "    if (word_wo_dash in words_u) and (\"-\" in word) :\n",
    "        count += 1\n",
    "        \n",
    "        det_0 = all([len(w) > 5 for w in word.split(\" \") if \"-\" in w])\n",
    "        det_1 = all([len(w) >= 2 for w in word.split(\"-\")]) \n",
    "#         print(count, word, det_0, det_1)\n",
    "        \n",
    "        if det_0 and det_1:\n",
    "            keywords_dash_update.update({word_wo_dash: word})\n",
    "\n",
    "# print(keywords_dash_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash_update_keys = list(keywords_dash_update.keys())\n",
    "dict_values_update_ = deepcopy(dict_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, d in enumerate(dict_values_update):\n",
    "    for key in dash_update_keys:\n",
    "        if d == key:\n",
    "            count += 1\n",
    "            dict_values_update_[i] = keywords_dash_update[key]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update keywords based on `frequency` (top 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
