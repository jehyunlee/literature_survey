{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re, nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus = pd.read_csv(\"scopus.csv\")\n",
    "keyword_org_name = \"keywords.txt\"\n",
    "keyword_abb_name = \"keyword_abb.json\"\n",
    "keyword_dash_name = \"keywords_dash.txt\"\n",
    "keyword_dict_name = \"keywords_dict.json\"\n",
    "keyword_singe_name = \"keywords_single.txt\"\n",
    "keyword_plural_name = \"keywords_plural.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "line = ''\n",
    "\n",
    "with open(\"BIPV_ML_all.txt\") as f:\n",
    "    while line != \"EF\":\n",
    "        line = f.readline()\n",
    "        if line[:3] == \"DE \":\n",
    "            keywords_new = line[3:].rstrip('\\n').split(\";\")\n",
    "            for keyword_new in keywords_new:\n",
    "                keywords += [keyword_new.rstrip(' ').lstrip(' ')]\n",
    "\n",
    "keywords = sorted(keywords)\n",
    "with open(keyword_org_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords:\n",
    "        f.write(f\"{keyword}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1565\n",
      "[[1, '% free psa (% fpsa)', 'PSA', '% free psa'], [2, '% free psa (% fpsa)', 'FPSA', '% free psa'], [3, '2000-w/1-t co2 society vision', 'CO2', '2000-w/1-t co2 society vision'], [4, 'co2 direct air capture', 'CO2', 'co2 direct air capture'], [5, 'co2 emission', 'CO2', 'co2 emission'], [6, 'co2 tax', 'CO2', 'co2 tax'], [7, 'dc/dc-boost-converter', 'DC', 'dc/dc-boost-converter'], [8, '5-level dcmi', 'DC', '5-level dcmi'], [9, 'ac/dc grid', 'DC', 'ac/dc grid'], [10, 'dc pump', 'DC', 'dc pump'], [11, 'dc bus', 'DC', 'dc bus'], [12, '(epv,qd) operating points', 'PV', 'operating points'], [13, 'bipv system', 'PV', 'bipv system'], [14, 'cigs pv', 'PV', 'cigs pv'], [15, '3d gis', 'GIS', '3d gis'], [16, '5-level dcmi', 'DCMI', '5-level dcmi'], [17, 'a vr', 'VR', 'a vr'], [18, 'aa%', 'AA%', 'aa%'], [19, 'abc', 'ABC', 'abc']]\n"
     ]
    }
   ],
   "source": [
    "# retreive keywords\n",
    "keywords_raw_ = np.genfromtxt(\"keywords.txt\", dtype=\"str\", delimiter=\"\\n\", encoding='utf-8-sig') #utf-8-sig for \\ufeff removal\n",
    "\n",
    "# get unique ones\n",
    "keywords_raw_ = np.unique(keywords_raw_)\n",
    "\n",
    "# abbriviations detection (key candidates)\n",
    "keywords_abb = {}\n",
    "keywords_abb_key_cand = []\n",
    "for keyword_raw_ in keywords_raw_:\n",
    "    kwr = keyword_raw_.split()\n",
    "    for kw in kwr:\n",
    "        if (kw == kw.upper()) \\\n",
    "           and (kw.lstrip(\"(\").rstrip(\")\") not in keywords_abb_key_cand) \\\n",
    "           and (ord(kw[0]) >= ord('A') and ord(kw[0]) <= ord('Z')) \\\n",
    "           and (len(kw) > 1):\n",
    "            keywords_abb_key_cand.append(kw.lstrip(\"(\").rstrip(\")\"))\n",
    "\n",
    "# convert to lower cases\n",
    "keywords_raw = [kw.lower() for kw in keywords_raw_]\n",
    "\n",
    "# duplicate for values\n",
    "keywords_value = deepcopy(keywords_raw)\n",
    "\n",
    "### keywords cleaning\n",
    "keywords_dash = [] # additional dictionary, for words containing '-'\n",
    "\n",
    "# remove staring characters\n",
    "keywords_value = [kw.lstrip('-') for kw in keywords_value]\n",
    "keywords_value = [kw.rstrip('-') for kw in keywords_value]\n",
    "keywords_value = [kw.lstrip(':') for kw in keywords_value]\n",
    "keywords_value = [kw.rstrip(':') for kw in keywords_value]\n",
    "\n",
    "# remove brakets \"()\"\n",
    "count = 0\n",
    "tmp = []\n",
    "for kc in keywords_abb_key_cand:\n",
    "    kc_ = kc.lower()\n",
    "    for kw in keywords_value:\n",
    "        if f\"{kc_}\" in kw:\n",
    "            kc_value = re.sub(r'\\([^)]*\\)', \"\", kw).rstrip(' ').lstrip(' ').rstrip(\",\").rstrip(\":\")\n",
    "            if (kc not in list(keywords_abb.keys())) or (kc in list(keywords_abb.keys()) and len(kc_value) < len(keywords_abb[kc]) and len(kc_value.split()) > 1):\n",
    "                keywords_abb.update({kc:kc_value})\n",
    "                count += 1\n",
    "                if count < 20:\n",
    "                    tmp.append([count, kw, kc, kc_value])\n",
    "\n",
    "print(count)\n",
    "print(tmp)\n",
    "keywords_value = [re.sub(r'\\([^)]*\\)', \"\", kw).rstrip(' ').lstrip(' ') for kw in keywords_value]\n",
    "\n",
    "# remove unicodes –\n",
    "keywords_value = [kw.replace(\"‐\", \"-\") for kw in keywords_value] # \\u2010 hypen\n",
    "keywords_value = [kw.replace(\"–\", \"-\") for kw in keywords_value] # \\u2013 dash \n",
    "keywords_value = [kw.replace(\"—\", \"-\") for kw in keywords_value] # \\u2014 dash \n",
    "keywords_value = [kw.replace(\"’\", \"-\") for kw in keywords_value] # \\u2019 right quotation mark\n",
    "keywords_value = [kw.replace(\"…\", \"...\") for kw in keywords_value] # \\u2026 horizontal ellipsis\n",
    "\n",
    "# remove mathematics\n",
    "keywords_value = [kw.replace(\"\\\\infty\", \"infinity\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"\\\\mathrm{\", \"\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"{\", \"\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"}\", \"\") for kw in keywords_value]\n",
    "\n",
    "# remove '\"'\n",
    "keywords_value = [kw.replace('\"', \"\") for kw in keywords_value]\n",
    "\n",
    "# reduce needless blanks.\n",
    "keywords_value = [re.sub(\"\\s+\", \" \", kw) for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\" -\", \"-\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"- \", \"-\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\" //\", \"//\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"// \", \"//\") for kw in keywords_value]\n",
    "\n",
    "# create single-word keyword\n",
    "keywords_single = []\n",
    "for kw in keywords_value:\n",
    "    if len(kw.split()) == 1 and len(kw) <= 4:\n",
    "        keywords_single.append(kw)\n",
    "\n",
    "# Convert plural to singular\n",
    "Lem = WordNetLemmatizer()\n",
    "keywords_plural = {}\n",
    "keywords_value_ = []\n",
    "for i, kw in enumerate(keywords_value):\n",
    "    if (kw not in keywords_single) and (len(kw.split(' ')) > 1):\n",
    "        kw_ = ' '.join(kw.split(' ')[:-1] + [Lem.lemmatize(kw.split(' ')[-1])])\n",
    "        keywords_value_.append(kw_)\n",
    "    else:\n",
    "        keywords_value_.append(kw)\n",
    "        \n",
    "    kw_ = kw.split(' ')\n",
    "    for kw__ in kw_:\n",
    "        if kw__ not in keywords_single and Lem.lemmatize(kw__) != kw__:\n",
    "            keywords_plural.update({kw__: Lem.lemmatize(kw__)})\n",
    "\n",
    "keywords_value = deepcopy(keywords_value_)\n",
    "        \n",
    "    \n",
    "# keywords_value = [' '.join(kw.split(' ')[:-1] + [Lem.lemmatize(kw.split(' ')[-1])] if len(kw.split(' ')) > 1 and not in keywords_single else kw for kw in keywords_value]\n",
    "\n",
    "keywords_value = [kw.lstrip(' ').rstrip(' ') for kw in keywords_value]\n",
    "    \n",
    "# create dash dictionary\n",
    "for kw in keywords_value:\n",
    "    kw_ = kw.split(\" \")\n",
    "    for kw__ in kw_:\n",
    "        if ('-' in kw__) or ('//' in kw__):\n",
    "            keywords_dash.append(kw__)\n",
    "    \n",
    "keywords_dash = np.unique(keywords_dash)\n",
    "\n",
    "# create dictionary\n",
    "keywords_dict = dict(zip(keywords_raw, keywords_value))\n",
    "with open(keyword_dict_name, \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)\n",
    "    \n",
    "# create plural-singular dictionary\n",
    "plural_sorted = dict(sorted(keywords_plural.items()))\n",
    "with open(keyword_plural_name, \"w\") as j:\n",
    "    json.dump(plural_sorted, j, ensure_ascii=False, indent=2)\n",
    "\n",
    "# create abbreviation dictionary\n",
    "with open(keyword_abb_name, \"w\") as j:\n",
    "    json.dump(keywords_abb, j, ensure_ascii=False, indent=2)\n",
    "\n",
    "# create \"dash\" keywords list\n",
    "with open(keyword_dash_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords_dash:\n",
    "        f.write(f\"{keyword}\\n\")\n",
    "\n",
    "# create \"dash\" keywords list\n",
    "with open(keyword_singe_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords_single:\n",
    "        f.write(f\"{keyword}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"bsf\".split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial neural networks'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lem.lemmatize(\"networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cobbler\n",
      "ant\n",
      "woman\n",
      "boy\n",
      "need\n",
      "find\n",
      "binary\n",
      "hobby\n",
      "bus\n",
      "wolf\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "Lem = WordNetLemmatizer()\n",
    "\n",
    "phrase = 'cobblers ants women boys needs finds binaries hobbies busses wolves'\n",
    "\n",
    "words = phrase.split()\n",
    "for word in words :\n",
    "  lemword = Lem.lemmatize(word)\n",
    "  print(lemword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-d modeling: 1-d modeling\n",
      "airs total column ozone: airs total column ozone\n",
      "accuracy evaluation: accuracy evaluation\n",
      "advanced alkaline electrolyzer: advanced alkaline electrolyzer\n",
      "allocation: allocation\n",
      "antarctic stratospheric circumpolar vortex: antarctic stratospheric circumpolar vortex\n",
      "asynchrouns machine: asynchrouns machine\n",
      "bipv performance: bipv performance\n",
      "battery control algorithm: battery control algorithm\n",
      "best-so-far abc: best-so-far abc\n"
     ]
    }
   ],
   "source": [
    "for key in keywords_raw[10:1000:100]:\n",
    "    print(f\"{key}: {keywords_dict[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* edit `values` while keeping `keys`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. words inside bracket removal\n",
    "[re.sub(r'\\([^)]*\\)', \"\", keyword_).rstrip(' ').lstrip(' ').lower() for keyword_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['λ-ridge concentrators',\n",
       " 'αβ0',\n",
       " 'β-blocker',\n",
       " 'ε-constraint and fuzzy decision making approaches',\n",
       " 'ε-constraint technique',\n",
       " 'μ',\n",
       " 'μ-synthesis',\n",
       " 'μ-analysis',\n",
       " 'π line model',\n",
       " '\\ufeffforecasting']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_raw[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
