{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re, nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import itertools\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "Lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus = pd.read_csv(\"scopus.csv\")\n",
    "keyword_org_name = \"keywords.txt\"\n",
    "keyword_abb_name = \"keywords_abb.json\"\n",
    "keyword_dash_name = \"keywords_dash.json\"\n",
    "keyword_dict_name = \"keywords_dict.json\"\n",
    "keyword_single_name = \"keywords_single.txt\"\n",
    "keyword_plural_name = \"keywords_plural.json\"\n",
    "unicode_name = \"unicode_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unicode dictionary\n",
    "with open('unicode_dict.json') as j:\n",
    "    unicode_dict = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "line = ''\n",
    "\n",
    "with open(\"BIPV_ML_all.txt\") as f:\n",
    "    while line != \"EF\":\n",
    "        line = f.readline()\n",
    "        if line[:3] == \"DE \":\n",
    "            keywords_new = line[3:].rstrip('\\n').split(\";\")\n",
    "            for keyword_new in keywords_new:\n",
    "                keywords += [keyword_new.rstrip(' ').lstrip(' ')]\n",
    "\n",
    "keywords = sorted(keywords)\n",
    "with open(keyword_org_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords:\n",
    "        f.write(f\"{keyword}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aerosols\n",
      "aggregators\n",
      "agrivoltaics\n",
      "algorithms\n",
      "barriers\n",
      "bifurcations\n",
      "buildings\n",
      "characteristics\n",
      "choppers\n",
      "clouds\n",
      "consumers\n",
      "controllers\n",
      "converters\n",
      "correlations\n",
      "costs\n",
      "delays\n",
      "dielectrics\n",
      "diodes\n",
      "dislocations\n",
      "dynamics\n",
      "economics\n",
      "eigenvalues\n",
      "electrolyzers\n",
      "emissions\n",
      "ensembles\n",
      "experiments\n",
      "failures\n",
      "faults\n",
      "feed-in-tariffs\n",
      "forecasts\n",
      "greenhouses\n",
      "harmonics\n",
      "heterojunctions\n",
      "households\n",
      "hurricanes\n",
      "ibscs\n",
      "igbts\n",
      "imports\n",
      "inverters\n",
      "irradiances\n",
      "measurements\n",
      "metaheuristics\n",
      "micro-grids\n",
      "microgrids\n",
      "microinverters\n",
      "microturbines\n",
      "mini-grids\n",
      "modes\n",
      "models\n",
      "modules\n",
      "multi-junctions\n",
      "nano-grids\n",
      "nanofluids\n",
      "nanowires\n",
      "optoelectronics\n",
      "performances\n",
      "perovskites\n",
      "photodetectors\n",
      "photodiodes\n",
      "photovoltaics\n",
      "prosumers\n",
      "relays\n",
      "renewables\n",
      "resonances\n",
      "scenarios\n",
      "semiconductors\n",
      "sensors\n",
      "simulations\n",
      "simulators\n",
      "supercapacitors\n",
      "systems\n",
      "techno-economics\n",
      "technoeconomics\n",
      "thermodynamics\n",
      "thermophotovoltaics\n",
      "transients\n",
      "trends\n",
      "voltages\n",
      "waves\n",
      "wavelets\n"
     ]
    }
   ],
   "source": [
    "# retreive keywords\n",
    "keywords_raw_ = np.genfromtxt(\"keywords.txt\", dtype=\"str\", delimiter=\"\\n\", encoding='utf-8-sig') #utf-8-sig for \\ufeff removal\n",
    "\n",
    "# get unique ones\n",
    "keywords_raw = np.unique(keywords_raw_)\n",
    "\n",
    "# abbriviations detection (key candidates)\n",
    "keywords_abb = {}\n",
    "keywords_abb_key_cand = []\n",
    "for keyword_raw_ in keywords_raw_:\n",
    "    kwr = keyword_raw_.split()\n",
    "    for kw in kwr:\n",
    "        kw = kw.lstrip(\"(\").rstrip(\")\")\n",
    "        if (kw == kw.upper()) \\\n",
    "           and (kw.lstrip(\"(\").rstrip(\")\") not in keywords_abb_key_cand) \\\n",
    "           and (ord(kw[0]) >= ord('A') and ord(kw[0]) <= ord('Z')) \\\n",
    "           and (len(kw) > 1):\n",
    "            keywords_abb_key_cand.append(kw.lstrip(\"(\").rstrip(\")\").rstrip(\":\").rstrip(\",\"))\n",
    "\n",
    "# convert to lower cases\n",
    "# keywords_raw = [kw.lower() for kw in keywords_raw_]\n",
    "\n",
    "# duplicate for values, and convert to lower cases\n",
    "keywords_value = deepcopy(keywords_raw)\n",
    "\n",
    "# convert to lower cases\n",
    "keywords_value = [kw.lower() for kw in keywords_value]\n",
    "\n",
    "# remove staring characters\n",
    "keywords_value = [kw.lstrip('-') for kw in keywords_value]\n",
    "keywords_value = [kw.rstrip('-') for kw in keywords_value]\n",
    "keywords_value = [kw.lstrip(':') for kw in keywords_value]\n",
    "keywords_value = [kw.rstrip(':') for kw in keywords_value]\n",
    "\n",
    "# remove brakets \"()\"\n",
    "for kc in keywords_abb_key_cand:\n",
    "    kc_ = kc.lower()\n",
    "    for kw in keywords_value:\n",
    "        if kc_ in kw.replace('(', \"\").replace(')', \"\").split():\n",
    "            kc_value = re.sub(r'\\([^)]*\\)', \"\", kw).rstrip(' ').lstrip(' ')\n",
    "            if (kc_ not in kc_value) and \\\n",
    "               (kc_value[-1] != \")\") and \\\n",
    "               (len(kc_value.split()) > 1) and \\\n",
    "               ((kc not in list(keywords_abb.keys())) or (kc in list(keywords_abb.keys()) and len(kc_value) < len(keywords_abb[kc]) and len(kc_value.split()) > 1)):\n",
    "                kc_value = kc_value.replace(\"  \", \" \")\n",
    "                kc_value = ' '.join(kc_value.split(' ')[:-1] + [Lem.lemmatize(kc_value.split(' ')[-1])])\n",
    "                keywords_abb.update({kc:kc_value})\n",
    "\n",
    "keywords_value = [re.sub(r'\\([^)]*\\)', \"\", kw).rstrip(' ').lstrip(' ').lstrip(')').lstrip('(') for kw in keywords_value]\n",
    "\n",
    "# remove unicodes â€“\n",
    "unicode_keys = list(unicode_dict.keys())\n",
    "for ukey in unicode_keys:\n",
    "        keywords_value = [kw.replace(ukey, unicode_dict[ukey]) for kw in keywords_value]\n",
    "\n",
    "# remove mathematics\n",
    "keywords_value = [kw.replace(\"\\\\infty\", \"infinity\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"\\\\mathrm{\", \"\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"{\", \"\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"}\", \"\") for kw in keywords_value]\n",
    "\n",
    "# remove '\"'\n",
    "keywords_value = [kw.replace('\"', \"\") for kw in keywords_value]\n",
    "\n",
    "# reduce needless blanks.\n",
    "keywords_value = [re.sub(\"\\s+\", \" \", kw) for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\" -\", \"-\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"- \", \"-\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\" //\", \"//\") for kw in keywords_value]\n",
    "keywords_value = [kw.replace(\"// \", \"//\") for kw in keywords_value]\n",
    "\n",
    "# create single-word keyword\n",
    "keywords_single = [k for k in list(keywords_abb.keys())]\n",
    "for kws in keywords_value:\n",
    "    if len(kws.split(\" \")) == 1 and len(kw) <= 4:\n",
    "        keywords_single.append(kws)\n",
    "    for kw in kws:\n",
    "        if len(kw.split(\"-\")) == 1 and len(kw) <= 4 and len(kw) > 1:\n",
    "            keywords_single.append(kw)\n",
    "\n",
    "keywords_single = list(np.unique(sorted(keywords_single)))\n",
    "\n",
    "keywords_single_ = deepcopy(keywords_single)       \n",
    "for kw in keywords_single_:\n",
    "    if kw+\"s\" in keywords_single and len(kw) >= 4:\n",
    "        print(kw+\"s\")\n",
    "        keywords_single.remove(kw+\"s\")\n",
    "    if kw[-1]+\"ies\" in keywords_single and kw[-1] == \"y\" and len(kw >=4):\n",
    "        print(kw+\"s\")\n",
    "        keywords_single.remove(kw+\"s\")\n",
    "\n",
    "# create abbreviation dictionary\n",
    "abb_sorted = dict(sorted(keywords_abb.items()))\n",
    "with open(keyword_abb_name, \"w\") as j:\n",
    "    json.dump(abb_sorted, j, ensure_ascii=False, indent=2)\n",
    "    \n",
    "# create \"single\" keywords list\n",
    "with open(keyword_single_name, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for keyword in keywords_single:\n",
    "        f.write(f\"{keyword}\\n\")\n",
    "\n",
    "# create \"value\"\n",
    "keywords_dict = dict(zip(keywords_raw, keywords_value))\n",
    "with open(keyword_dict_name, \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual refine `keywords_single`\n",
    "* plural words not to be kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve keywords_value\n",
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)\n",
    "keywords_value = list(keywords_dict.values())\n",
    "keywords_key = list(keywords_dict.keys())\n",
    "\n",
    "# retreive keywords_single\n",
    "keywords_single = np.genfromtxt(\"keywords_single_refine.txt\", dtype=\"str\", delimiter=\"\\n\", encoding='utf-8-sig') #utf-8-sig for \\ufeff removal\n",
    "keywords_single = [k.lower() for k in keywords_single]\n",
    "\n",
    "# Convert plural to singular\n",
    "Lem = WordNetLemmatizer()\n",
    "keywords_plural = {}\n",
    "keywords_value_ = []\n",
    "for kws in keywords_value:\n",
    "    if (kws not in keywords_single) and (len(kws.split(\" \")) > 1):\n",
    "        kws_0 = kws.split(\" \")\n",
    "        \n",
    "        for kws_1 in kws_0:\n",
    "            kws_2 = kws_1.split(\"-\")\n",
    "            for kw in kws_2:\n",
    "                if (kw not in keywords_single) and (kw != Lem.lemmatize(kw)):\n",
    "                    kws = kws.replace(kw, Lem.lemmatize(kw))\n",
    "                    keywords_plural.update({kw: Lem.lemmatize(kw)})\n",
    "        keywords_value_.append(kws)\n",
    "    else:\n",
    "        keywords_value_.append(kws)\n",
    "\n",
    "keywords_value = deepcopy(keywords_value_)\n",
    "keywords_value = [kw.lstrip(' ').rstrip(' ') for kw in keywords_value]\n",
    "\n",
    "    \n",
    "# create plural-singular dictionary\n",
    "plural_sorted = dict(sorted(keywords_plural.items()))\n",
    "with open(keyword_plural_name, \"w\") as j:\n",
    "    json.dump(plural_sorted, j, ensure_ascii=False, indent=2)\n",
    "    \n",
    "# create \"value\"\n",
    "keywords_dict = dict(zip(keywords_key, keywords_value))\n",
    "with open(keyword_dict_name, \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve keywords_value\n",
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)\n",
    "keywords_value = list(keywords_dict.values())\n",
    "\n",
    "# create dash dictionary\n",
    "keywords_dash = []\n",
    "for kw in keywords_value:\n",
    "    kw_ = kw.split(\" \")\n",
    "    for kw__ in kw_:\n",
    "        if ('-' in kw__) or ('//' in kw__):\n",
    "            keywords_dash.append(kw__)\n",
    "    \n",
    "keywords_dash = np.unique(keywords_dash)\n",
    "\n",
    "# create \"dash\" keywords dictionary - manual refinement required\n",
    "keywords_dash_dict = dict(zip(keywords_dash, keywords_dash))\n",
    "with open(keyword_dash_name, \"w\") as j:\n",
    "    json.dump(keywords_dash_dict, j, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual keywords-abb refinement\n",
    "* incorrect names, typo errors\n",
    "* unifying same words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual keyworkds-dash refinement\n",
    "* incorrect names, typo errors\n",
    "* unifying same words (ex. photo-voltaic and photovoltaic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_plural.json\", \"r\") as j:\n",
    "    keywords_plural = json.load(j)\n",
    "\n",
    "with open(\"keywords_abb_refine.json\", \"r\") as j:\n",
    "    keywords_abb_refine = json.load(j)\n",
    "\n",
    "with open(\"keywords_dash_refine.json\", \"r\") as j:\n",
    "    keywords_dash_refine = json.load(j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "abb_refine_keys = list(keywords_abb_refine.keys())\n",
    "dash_refine_keys = list(keywords_dash_refine.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dash_refine2 = {}\n",
    "\n",
    "plural_keys = list(keywords_plural.keys())\n",
    "\n",
    "for dkey in dash_refine_keys:\n",
    "    dvalues = keywords_dash_refine[dkey].split(\"; \")\n",
    "    \n",
    "    dvalues_abb = []\n",
    "    dvalues_rms = []\n",
    "    for dvalue in dvalues:\n",
    "        dvalue_rem = deepcopy(dvalue)\n",
    "        for akey in abb_refine_keys:\n",
    "            if dvalue == akey.lower():\n",
    "                dvalues.remove(dvalue)\n",
    "                dvalues.append(keywords_abb_refine[akey])\n",
    "            else:\n",
    "                [dvalues_abb.append(keywords_abb_refine[d.upper()]) for d in dvalue.split(\"-\") if d == akey.lower()]\n",
    "                [dvalues_abb.append(keywords_abb_refine[d.upper()]) for d in dvalue.split(\"/\") if d == akey.lower()]\n",
    "\n",
    "    keywords_dash_refine2.update({dkey:dvalues+dvalues_abb})\n",
    "\n",
    "with open(\"keywords_dash_refine2.json\", \"w\") as j:\n",
    "    json.dump(keywords_dash_refine2, j, ensure_ascii=False, indent=2)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual keywords_dash2 refinement\n",
    "* incorrect abbs treatement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply keywords-dash on keywords_value\n",
    "* (1) replace keywords_dash_refine.keys() to keywords_dash_refine.values()\n",
    "* (2) if keywords_dash_refine2.key() has more than 2 elements, add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_dash_refine2.json\", \"r\") as j:\n",
    "    keywords_dash_refine2 = json.load(j)\n",
    "    \n",
    "\n",
    "keywords_value_ = []\n",
    "for kws in keywords_value:\n",
    "    dash_flag = 0\n",
    "    kws_ = []\n",
    "    for dkey in dash_refine_keys:\n",
    "        \n",
    "        if dkey in kws:\n",
    "            kws_ += [kws.replace(dkey, keywords_dash_refine[dkey])]\n",
    "            dash_flag += 1\n",
    "            \n",
    "            if len(keywords_dash_refine2[dkey]) > 1:\n",
    "                [kws_.append(v) for v in keywords_dash_refine2[dkey][1:]]\n",
    "                \n",
    "    if dash_flag == 0:\n",
    "        kws_ = [kws]\n",
    "    \n",
    "    keywords_value_.append(list(np.unique(kws_)))\n",
    "\n",
    "keywords_dict = dict(zip(list(keywords_dict.keys()), keywords_value_))\n",
    "with open(keyword_dict_name, \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find and extract dash and abbriviations\n",
    "* create `keywords_dict_add.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 46s, sys: 78.2 ms, total: 6min 46s\n",
      "Wall time: 6min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "\n",
    "abb_refine_keys = [k.lower() for k in np.unique(list(keywords_abb_refine.keys()))]\n",
    "abb_refine_values = [v.lower() for v in np.unique(list(keywords_abb_refine.values()))]\n",
    "\n",
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)    \n",
    "keywords_value = list(keywords_dict.values())\n",
    "\n",
    "keywords_value_add = []\n",
    "for kw in keywords_value:\n",
    "    kw_0 = kw\n",
    "    kw_1 = list(itertools.chain.from_iterable([kw_.split() for kw_ in kw]))\n",
    "    kw_2 = list(itertools.chain.from_iterable([kw_.split(\"-\") for kw_ in kw]))\n",
    "    kw_3 = list(itertools.chain.from_iterable([kw_.split(\"/\") for kw_ in kw]))\n",
    "    \n",
    "    keywords_value_add_ = []\n",
    "    \n",
    "    # \"dash\"\n",
    "    for dash_keys in dash_refine_keys:\n",
    "        if dash_keys in kw_1:\n",
    "            [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "        \n",
    "        dash_keys_ = dash_keys.replace(\"-\", \"\")\n",
    "        if dash_keys_ in kw_1:\n",
    "            if not (dash_keys_ in abb_refine_keys): # original abbriviation should be kept\n",
    "                [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "\n",
    "        dash_keys_ = dash_keys.split(\"-\")\n",
    "        if not (dash_keys_ in abb_refine_keys): # original abbriviation should be kept\n",
    "            try:\n",
    "                tmp = [kw_1.index(dash_key_) for dash_key_ in dash_keys_]\n",
    "                if tmp == sorted(tmp) and (len(np.unique(tmp)) == len(tmp)):\n",
    "                    [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    tmp = [kw_2.index(dash_key_) for dash_key_ in dash_keys_]\n",
    "                    if tmp == sorted(tmp) and (len(np.unique(tmp)) == len(tmp)):\n",
    "                        [keywords_value_add_.append(kw) for kw in keywords_dash_refine2[dash_keys]]\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    \n",
    "    # \"abb\"\n",
    "    for abb_vals in abb_refine_values:\n",
    "        if abb_vals in kw and \\\n",
    "           (abb_vals.replace(\" \", \"-\") not in keywords_value_add) and \\\n",
    "           (abb_vals.replace(\"-\", \" \") not in keywords_value_add):\n",
    "            keywords_value_add_.append(abb_vals)\n",
    "    \n",
    "    for abb_keys in abb_refine_keys:\n",
    "        if abb_keys in (kw_0 + kw_1 + kw_2 + kw_3):\n",
    "            keywords_value_add_.append(keywords_abb_refine[abb_keys.upper()])\n",
    "    \n",
    "    keywords_value_add.append(list(np.unique(keywords_value_add_)))\n",
    "\n",
    "# create \"additional\" keywords dictionary\n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, keywords_value_add))\n",
    "with open(\"keywords_dict_add_dashabb.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update `keywords_manual` on `keywords_dict` and `keywords_dict_add_dashabb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_manual.json\", \"r\") as j:\n",
    "    keywords_manual = json.load(j)\n",
    "    \n",
    "with open(\"keywords_dict_add_dashabb.json\", \"r\") as j:\n",
    "    keywords_dict_add_dashabb = json.load(j)\n",
    "    \n",
    "with open(\"keywords_dict.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_keys = list(keywords_manual.keys())\n",
    "dict_values = list(keywords_dict.values())\n",
    "add_dashabb_values = list(keywords_dict_add_dashabb.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values_update = []\n",
    "\n",
    "for values in dict_values:\n",
    "    value_new = []\n",
    "    for value in values:\n",
    "        for mkey in manual_keys:\n",
    "            mkey_ = mkey.lower()\n",
    "            value = value.replace(mkey_, keywords_manual[mkey])\n",
    "        value_new.append(value)\n",
    "    dict_values_update.append(value_new)\n",
    "    \n",
    "keywords_dict = dict(zip(keywords_raw, dict_values_update))\n",
    "with open(\"keywords_dict2.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dashabb_values_update = []\n",
    "\n",
    "for values in add_dashabb_values:\n",
    "    value_new = []\n",
    "    for value in values:\n",
    "        for mkey in manual_keys:\n",
    "            mkey_ = mkey.lower()\n",
    "            value = value.replace(mkey_, keywords_manual[mkey])\n",
    "        value_new.append(value)\n",
    "    add_dashabb_values_update.append(value_new)\n",
    "    \n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, add_dashabb_values_update))\n",
    "with open(\"keywords_dict_add_dashabb2.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check and remove almost same words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10224</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5291</th>\n",
       "      <td>generation</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8125</th>\n",
       "      <td>micro-grid</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11612</th>\n",
       "      <td>pv-system</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>energy-storage</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>irradiance</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5601</th>\n",
       "      <td>grid-connected</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10982</th>\n",
       "      <td>power-point</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>maximum-power-point</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>mpp tracker</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keywords  counts\n",
       "10224         photovoltaic    1731\n",
       "5291            generation     257\n",
       "8125            micro-grid     155\n",
       "11612            pv-system     112\n",
       "4287        energy-storage     108\n",
       "6915            irradiance     106\n",
       "5601        grid-connected      92\n",
       "10982          power-point      91\n",
       "7930   maximum-power-point      85\n",
       "8585           mpp tracker      85"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "merged = list(itertools.chain.from_iterable([dict_values_update, add_dashabb_values_update]))\n",
    "merged = list(itertools.chain.from_iterable(merged))\n",
    "merged_u = np.unique(merged, return_counts=True)\n",
    "\n",
    "words_u = merged_u[0]\n",
    "   \n",
    "\n",
    "df_words = pd.DataFrame({\"keywords\": merged_u[0],\n",
    "                         \"counts\": merged_u[1]\n",
    "                        }).sort_values(\"counts\", ascending=False)\n",
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) same words with and without dash(\"-\"): insert dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "keywords_dash_update = {}\n",
    "\n",
    "count = 0\n",
    "for word in words_u:\n",
    "    word_wo_dash = word.replace(\"-\", \"\")\n",
    "    if (word_wo_dash in words_u) and (\"-\" in word) :\n",
    "        count += 1\n",
    "        \n",
    "        det_0 = all([len(w) > 5 for w in word.split(\" \") if \"-\" in w])\n",
    "        det_1 = all([len(w) >= 2 for w in word.split(\"-\")]) \n",
    "#         print(count, word, det_0, det_1)\n",
    "        \n",
    "        if det_0 and det_1:\n",
    "            keywords_dash_update.update({word_wo_dash: word})\n",
    "\n",
    "# print(keywords_dash_update)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ac microgrid',\n",
       " 'antireflection',\n",
       " 'antireflection coating',\n",
       " 'asymmetrical fault',\n",
       " 'bidirectional',\n",
       " 'bidirectional converter',\n",
       " 'bidirectional dc/dc converter',\n",
       " 'bifacial',\n",
       " 'cogeneration',\n",
       " 'cosimulation']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(keywords_dash_update.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n"
     ]
    }
   ],
   "source": [
    "dash_update_keys = list(keywords_dash_update.keys())\n",
    "dict_values_update_ = deepcopy(dict_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(dict_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in dash_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                dict_values_update_[i][j] = keywords_dash_update[key]\n",
    "\n",
    "print(count)\n",
    "dict_values_update = deepcopy(dict_values_update_)\n",
    "\n",
    "keywords_dict = dict(zip(keywords_raw, dict_values_update_))\n",
    "with open(\"keywords_dict3.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "dash_update_keys = list(keywords_dash_update.keys())\n",
    "add_dashabb_values_update_ = deepcopy(add_dashabb_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(add_dashabb_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in dash_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                add_dashabb_values_update_[i][j] = keywords_dash_update[key]\n",
    "\n",
    "print(count)\n",
    "add_dashabb_values_update = deepcopy(add_dashabb_values_update_)\n",
    "\n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, add_dashabb_values_update_))\n",
    "with open(\"keywords_dict_add_dashabb3.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10153</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>generation</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8089</th>\n",
       "      <td>micro-grid</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>pv-system</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>energy-storage</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>irradiance</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5575</th>\n",
       "      <td>grid-connected</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10907</th>\n",
       "      <td>power-point</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>maximum-power-point</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8542</th>\n",
       "      <td>mpp tracker</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keywords  counts\n",
       "10153         photovoltaic    1731\n",
       "5265            generation     257\n",
       "8089            micro-grid     167\n",
       "11536            pv-system     112\n",
       "4266        energy-storage     108\n",
       "6884            irradiance     106\n",
       "5575        grid-connected      93\n",
       "10907          power-point      91\n",
       "7894   maximum-power-point      85\n",
       "8542           mpp tracker      85"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = list(itertools.chain.from_iterable([dict_values_update_, add_dashabb_values_update_]))\n",
    "merged = list(itertools.chain.from_iterable(merged))\n",
    "merged_u = np.unique(merged, return_counts=True)\n",
    "\n",
    "words_u = merged_u[0]\n",
    "   \n",
    "\n",
    "df_words = pd.DataFrame({\"keywords\": merged_u[0],\n",
    "                         \"counts\": merged_u[1]\n",
    "                        }).sort_values(\"counts\", ascending=False)\n",
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) same words with and without space(\" \"): remove space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "keywords_space_update = {}\n",
    "\n",
    "count = 0\n",
    "for word in words_u:\n",
    "    word_wo_space = word.replace(\" \", \"\")\n",
    "    if (word_wo_space in words_u) and (\" \" in word) :\n",
    "        count += 1\n",
    "        \n",
    "        det_0 = all([len(w) > 5 for w in word.split(\" \") if \"-\" in w])\n",
    "        det_1 = all([len(w) >= 2 for w in word.split(\"-\")]) \n",
    "#         print(count, word, det_0, det_1)\n",
    "        \n",
    "        if det_0 and det_1:\n",
    "            keywords_space_update.update({word: word_wo_space})\n",
    "\n",
    "# print(keywords_space_update)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto regressive': 'autoregressive',\n",
       " 'back propagation': 'backpropagation',\n",
       " 'co2 emission': 'co2emission',\n",
       " 'dig silent': 'digsilent',\n",
       " 'energy plan': 'energyplan',\n",
       " 'energy plus': 'energyplus',\n",
       " 'global grid': 'globalgrid',\n",
       " 'hydro power': 'hydropower',\n",
       " 'iec 61850': 'iec61850',\n",
       " 'lab view': 'labview',\n",
       " 'levenberg- marquardt': 'levenberg-marquardt',\n",
       " 'light gbm': 'lightgbm',\n",
       " 'mat lab': 'matlab',\n",
       " 'matlab / simulink': 'matlab/simulink',\n",
       " 'matlab/ simulink': 'matlab/simulink',\n",
       " 'micro controller': 'microcontroller',\n",
       " 'micro converter': 'microconverter',\n",
       " 'micro generation': 'microgeneration',\n",
       " 'micro source': 'microsource',\n",
       " 'nano fluid': 'nanofluid',\n",
       " 'p o': 'po',\n",
       " 'perturb & observe': 'perturb&observe',\n",
       " 'photo voltaic': 'photovoltaic',\n",
       " 'photovoltaic/ thermal': 'photovoltaic/thermal',\n",
       " 'power hardware-in-the-loop': 'powerhardware-in-the-loop',\n",
       " 'ret screen': 'retscreen',\n",
       " 'smart grid': 'smartgrid'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_space_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "with open(\"keywords_dict3.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)    \n",
    "    \n",
    "space_update_keys = list(keywords_space_update.keys())\n",
    "dict_values_update_ = deepcopy(dict_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(dict_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in space_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                dict_values_update_[i][j] = keywords_space_update[key]\n",
    "\n",
    "print(count)\n",
    "dict_values_update = deepcopy(dict_values_update_)\n",
    "\n",
    "keywords_dict = dict(zip(keywords_raw, dict_values_update_))\n",
    "with open(\"keywords_dict3.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open(\"keywords_dict_add_dashabb3.json\", \"r\") as j:\n",
    "    keywords_dict_dashabb3 = json.load(j)  \n",
    "\n",
    "dash_update_keys = list(keywords_dash_update.keys())\n",
    "add_dashabb_values_update_ = deepcopy(add_dashabb_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(add_dashabb_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in dash_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                add_dashabb_values_update_[i][j] = keywords_dash_update[key]\n",
    "\n",
    "print(count)\n",
    "add_dashabb_values_update = deepcopy(add_dashabb_values_update_)\n",
    "\n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, add_dashabb_values_update_))\n",
    "with open(\"keywords_dict_add_dashabb3.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10133</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5260</th>\n",
       "      <td>generation</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8072</th>\n",
       "      <td>micro-grid</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11514</th>\n",
       "      <td>pv-system</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>energy-storage</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6876</th>\n",
       "      <td>irradiance</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>grid-connected</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>power-point</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7881</th>\n",
       "      <td>maximum-power-point</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8525</th>\n",
       "      <td>mpp tracker</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keywords  counts\n",
       "10133         photovoltaic    1736\n",
       "5260            generation     257\n",
       "8072            micro-grid     167\n",
       "11514            pv-system     112\n",
       "4261        energy-storage     108\n",
       "6876            irradiance     106\n",
       "5569        grid-connected      93\n",
       "10885          power-point      91\n",
       "7881   maximum-power-point      85\n",
       "8525           mpp tracker      85"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = list(itertools.chain.from_iterable([dict_values_update_, add_dashabb_values_update_]))\n",
    "merged = list(itertools.chain.from_iterable(merged))\n",
    "merged_u = np.unique(merged, return_counts=True)\n",
    "\n",
    "words_u = merged_u[0]\n",
    "   \n",
    "\n",
    "df_words = pd.DataFrame({\"keywords\": merged_u[0],\n",
    "                         \"counts\": merged_u[1]\n",
    "                        }).sort_values(\"counts\", ascending=False)\n",
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) same words with dash(\"-\") and space(\" \"): insert dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n"
     ]
    }
   ],
   "source": [
    "keywords_dash_update = {}\n",
    "\n",
    "count = 0\n",
    "for word in words_u:\n",
    "    word_wo_dash = word.replace(\"-\", \" \")\n",
    "    if (word_wo_dash in words_u) and (\"-\" in word) :\n",
    "        count += 1\n",
    "        \n",
    "        det_0 = all([len(w) > 5 for w in word.split(\" \") if \"-\" in w])\n",
    "        det_1 = all([len(w) >= 2 for w in word.split(\"-\")]) \n",
    "#         print(count, word, det_0, det_1)\n",
    "        \n",
    "        if det_0 and det_1:\n",
    "            keywords_dash_update.update({word_wo_dash: word})\n",
    "\n",
    "# print(keywords_dash_update)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent based model',\n",
       " 'agent based modeling',\n",
       " 'air conditioning',\n",
       " 'amorphous silicon',\n",
       " 'arc flash',\n",
       " 'back to back converter',\n",
       " 'black box model',\n",
       " 'black box modeling',\n",
       " 'black start',\n",
       " 'boost converter']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(keywords_dash_update.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n"
     ]
    }
   ],
   "source": [
    "with open(\"keywords_dict3.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)    \n",
    "    \n",
    "dash_update_keys = list(keywords_dash_update.keys())\n",
    "dict_values_update_ = deepcopy(dict_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(dict_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in dash_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                dict_values_update_[i][j] = keywords_dash_update[key]\n",
    "\n",
    "print(count)\n",
    "dict_values_update = deepcopy(dict_values_update_)\n",
    "\n",
    "keywords_dict = dict(zip(keywords_raw, dict_values_update_))\n",
    "with open(\"keywords_dict4.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n"
     ]
    }
   ],
   "source": [
    "with open(\"keywords_dict_add_dashabb3.json\", \"r\") as j:\n",
    "    keywords_dict_dashabb3 = json.load(j)  \n",
    "\n",
    "dash_update_keys = list(keywords_dash_update.keys())\n",
    "add_dashabb_values_update_ = deepcopy(add_dashabb_values_update)\n",
    "\n",
    "count = 0\n",
    "for i, vs in enumerate(add_dashabb_values_update):\n",
    "    for j, v in enumerate(vs):\n",
    "        for key in dash_update_keys:\n",
    "            if v == key:\n",
    "                count += 1\n",
    "                add_dashabb_values_update_[i][j] = keywords_dash_update[key]\n",
    "\n",
    "print(count)\n",
    "add_dashabb_values_update = deepcopy(add_dashabb_values_update_)\n",
    "\n",
    "keywords_dict_add_dashabb = dict(zip(keywords_raw, add_dashabb_values_update_))\n",
    "with open(\"keywords_dict_add_dashabb4.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_add_dashabb, j, ensure_ascii=False, indent=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9924</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>generation</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7901</th>\n",
       "      <td>micro-grid</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>energy-storage</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11270</th>\n",
       "      <td>pv-system</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7715</th>\n",
       "      <td>maximum-power-point</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>irradiance</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10144</th>\n",
       "      <td>photovoltaic-thermal</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13329</th>\n",
       "      <td>solar-photovoltaic</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>grid-connected</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   keywords  counts\n",
       "9924           photovoltaic    1736\n",
       "5159             generation     257\n",
       "7901             micro-grid     176\n",
       "4187         energy-storage     120\n",
       "11270             pv-system     118\n",
       "7715    maximum-power-point     113\n",
       "6737             irradiance     106\n",
       "10144  photovoltaic-thermal     103\n",
       "13329    solar-photovoltaic      96\n",
       "5450         grid-connected      96"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = list(itertools.chain.from_iterable([dict_values_update, add_dashabb_values_update]))\n",
    "merged = list(itertools.chain.from_iterable(merged))\n",
    "merged_u = np.unique(merged, return_counts=True)\n",
    "\n",
    "words_u = merged_u[0]\n",
    "   \n",
    "\n",
    "df_words = pd.DataFrame({\"keywords\": merged_u[0],\n",
    "                         \"counts\": merged_u[1]\n",
    "                        }).sort_values(\"counts\", ascending=False)\n",
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) apply `unique()` on values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords_dict4.json\", \"r\") as j:\n",
    "    keywords_dict = json.load(j)   \n",
    "dict_values = list(keywords_dict.values())\n",
    "dict_keys = list(keywords_dict.keys())\n",
    "\n",
    "with open(\"keywords_dict_add_dashabb4.json\", \"r\") as j:\n",
    "    keywords_dict_dashabb = json.load(j)  \n",
    "dict_dashabb_values = list(keywords_dict_dashabb.values())\n",
    "\n",
    "value_unique = []\n",
    "for i, (dict_value, dashabb_value) in enumerate(zip(dict_values, dict_dashabb_values)):\n",
    "    v1 = list(itertools.chain.from_iterable([v.split(\"; \") for v in dict_value]))\n",
    "    v2 = list(itertools.chain.from_iterable([v.split(\"; \") for v in dashabb_value]))\n",
    "    value_unique.append(list(set(v1 + v2)))\n",
    "    \n",
    "keywords_dict_u = dict(zip(keywords_raw, value_unique))\n",
    "with open(\"keywords_dict_u.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_u, j, ensure_ascii=False, indent=2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update keywords based on `frequency` (top 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>micro-grid</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pv-system</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>energy-storage</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>irradiance</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>maximum-power-point</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>power-point</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>photovoltaic-thermal</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>grid-connected</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               keywords  counts\n",
       "0          photovoltaic    1618\n",
       "1            generation     255\n",
       "2            micro-grid     146\n",
       "3             pv-system     111\n",
       "4        energy-storage     108\n",
       "5            irradiance     104\n",
       "6   maximum-power-point      96\n",
       "7           power-point      91\n",
       "8  photovoltaic-thermal      89\n",
       "9        grid-connected      89"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_u = list(itertools.chain.from_iterable(value_unique))\n",
    "value_u = np.unique(value_u, return_counts=True)\n",
    "df_words = pd.DataFrame({\"keywords\": value_u[0],\n",
    "                         \"counts\": value_u[1]\n",
    "                        }).sort_values(\"counts\", ascending=False).reset_index(drop=True)\n",
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "      <th>single</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>1618</td>\n",
       "      <td>[photovoltaic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation</td>\n",
       "      <td>255</td>\n",
       "      <td>[generation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>micro-grid</td>\n",
       "      <td>146</td>\n",
       "      <td>[micro-grid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pv-system</td>\n",
       "      <td>111</td>\n",
       "      <td>[pv-system]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>energy-storage</td>\n",
       "      <td>108</td>\n",
       "      <td>[energy-storage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>irradiance</td>\n",
       "      <td>104</td>\n",
       "      <td>[irradiance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>maximum-power-point</td>\n",
       "      <td>96</td>\n",
       "      <td>[maximum-power-point]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>power-point</td>\n",
       "      <td>91</td>\n",
       "      <td>[power-point]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>photovoltaic-thermal</td>\n",
       "      <td>89</td>\n",
       "      <td>[photovoltaic-thermal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>grid-connected</td>\n",
       "      <td>89</td>\n",
       "      <td>[grid-connected]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               keywords  counts                  single\n",
       "0          photovoltaic    1618          [photovoltaic]\n",
       "1            generation     255            [generation]\n",
       "2            micro-grid     146            [micro-grid]\n",
       "3             pv-system     111             [pv-system]\n",
       "4        energy-storage     108        [energy-storage]\n",
       "5            irradiance     104            [irradiance]\n",
       "6   maximum-power-point      96   [maximum-power-point]\n",
       "7           power-point      91           [power-point]\n",
       "8  photovoltaic-thermal      89  [photovoltaic-thermal]\n",
       "9        grid-connected      89        [grid-connected]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_kw(kw):\n",
    "    kw_space = kw.split(\" \")\n",
    "    return kw_space\n",
    "\n",
    "df_words[\"single\"] = df_words[\"keywords\"].apply(split_kw)\n",
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>photovoltaic</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>system</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>power</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>energy</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>solar</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>modeling</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>control</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>converter</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        keywords  counts\n",
       "0   photovoltaic     905\n",
       "1         system     788\n",
       "2          power     759\n",
       "3          model     677\n",
       "4         energy     635\n",
       "5          solar     607\n",
       "6       modeling     467\n",
       "8        control     350\n",
       "10     converter     232\n",
       "11        hybrid     224"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = [\"and\", \"of\", \"the\"]\n",
    "\n",
    "single_merged = list(itertools.chain.from_iterable(df_words[\"single\"].tolist()))\n",
    "single_merged_ = deepcopy(single_merged)\n",
    "\n",
    "for i, word in enumerate(single_merged):\n",
    "    if (word in abb_refine_keys) and (len(keywords_abb_refine[word.upper()].split(\" \"))==1):\n",
    "        single_merged_[i] = keywords_abb_refine[word.upper()]\n",
    "\n",
    "single_merged_u = np.unique(single_merged_, return_counts=True)        \n",
    "\n",
    "df_words_single = pd.DataFrame({\"keywords\": single_merged_u[0],\n",
    "                         \"counts\": single_merged_u[1]\n",
    "                        }).sort_values(\"counts\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "for sword in stop_words:\n",
    "    idx = df_words_single[df_words_single[\"keywords\"] == sword ].index\n",
    "    df_words_single.drop(idx, axis=0, inplace=True)\n",
    "\n",
    "df_words_single.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([133]),)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df_words_single[\"keywords\"]==\"shadow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([31]),)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df_words_single[\"keywords\"]==\"building\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1151]),)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df_words_single[\"keywords\"]==\"bipv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['photovoltaic', 'system', 'power', 'model', 'energy', 'solar', 'modeling', 'control', 'converter', 'hybrid', 'analysis', 'algorithm', 'network', 'optimization', 'prediction', 'load', 'generation', 'voltage', 'cell', 'inverter']\n",
      "['traffic', 'mosfet', 'volume', 'integral', 'positive', 'common-mode', 'common', 'explicit', 'arima', 'asset', 'decoupled', 'zinc', 'optoelectronic', 'phenomenon', 'porous', 'mission', '5-level', 'cavity', 'evacuated', 'assisted']\n"
     ]
    }
   ],
   "source": [
    "words_single_1200 = df_words_single[\"keywords\"].iloc[:1200].tolist()\n",
    "print(words_single_1200[:20])\n",
    "print(words_single_1200[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177]),)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df_words_single[\"counts\"]>30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_single_180 = df_words_single[\"keywords\"].iloc[:180].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract words in top 180 + bipv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19457\n"
     ]
    }
   ],
   "source": [
    "words_manual = [\"bipv\"]\n",
    "words_select = words_single_180 + words_manual\n",
    "\n",
    "freq_values = []\n",
    "for values in value_unique:\n",
    "    values_add = []\n",
    "    for w in words_select:\n",
    "        values_add.append([w for v in values if w in v.replace(\"-\", \" \").split(\" \")])\n",
    "    freq_values.append(list(set(itertools.chain.from_iterable(values_add))))\n",
    "\n",
    "print(len(freq_values))\n",
    "keywords_dict_freq = dict(zip(keywords_raw, freq_values))\n",
    "with open(\"keywords_dict_freq.json\", \"w\") as j:\n",
    "    json.dump(keywords_dict_freq, j, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['behavioural policy'],\n",
       " ['behind-the-meter solar', 'behind-the-meter'],\n",
       " ['behind-the-meter'],\n",
       " ['behind-the-meter',\n",
       "  'pv-storage',\n",
       "  'behind-the-meter pv and storage',\n",
       "  'photovoltaic'],\n",
       " ['behind-the-meter', 'solar-pv', 'photovoltaic', 'behind-the-meter solar pv'],\n",
       " ['belief'],\n",
       " ['benchmark model'],\n",
       " ['benchmark network'],\n",
       " ['benchmarking'],\n",
       " ['benchmarking comparison']]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_unique[900:910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['solar'],\n",
       " [],\n",
       " ['photovoltaic', 'storage'],\n",
       " ['solar', 'photovoltaic'],\n",
       " [],\n",
       " ['model'],\n",
       " ['network'],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_values[900:910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([90]),)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df_words_single[\"keywords\"]==\"mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing \"knowledge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_knowledge_init = {\n",
    "    \"statistics\": [\n",
    "        \"analysis_of_variance\", \n",
    "        \"autoregressive_integrated_moving_average\", \n",
    "        \"autoregressive_moving_average\", \n",
    "        \"markov\", \n",
    "        \"kalman_filter\", \n",
    "        \"bayes\", \n",
    "        \"gaussian_process\", \n",
    "        \"autoregressi\", \n",
    "        \"probabilistic\", \n",
    "        \"kfold\", \n",
    "        \"ensemble\", \n",
    "        \"kruskal_wallis\" \n",
    "    ], \n",
    "\n",
    "    \"method\": [\n",
    "        \"taguchi\", \n",
    "        \"averaging_point_method\", \n",
    "        \"describing_function_method\", \n",
    "        \"finite_difference_time_domain_method\", \n",
    "        \"finite_element_method\", \n",
    "        \"group_method_of_data_handling\", \n",
    "        \"incremental_conductance_method\", \n",
    "        \"match_evaluation_method\", \n",
    "        \"multiple_shifted_frequency_method\", \n",
    "        \"numerical_method\", \n",
    "        \"oblique_asymptote_method\", \n",
    "        \"response_surface_methodology\", \n",
    "        \"steepest_descent_method\", \n",
    "        \"nelder_mead\", \n",
    "        \"newton_raphson\", \n",
    "        \"runge_kutta\", \n",
    "        \"conditional_interpolation\", \n",
    "        \"expectation_maximization\", \n",
    "        \"dynamic_simulation\", \n",
    "        \"empirical\", \n",
    "        \"emulation\", \n",
    "        \"gradient_descent\", \n",
    "        \"algorithm\" \n",
    "    ],\n",
    "\n",
    "    \"metric\": [\n",
    "        \"mean_square_error\", \n",
    "        \"mean_absolute_error\", \n",
    "        \"mean_absolute_percentage_error\", \n",
    "        \"cross_entropy\", \n",
    "        \"least_square\"\n",
    "    ], \n",
    "\n",
    "    \"machine_learning\": [\n",
    "        \"clustering\", \n",
    "        \"regression\", \n",
    "        \"classification\", \n",
    "        \"dimension_reduction\", \n",
    "        \"reinforcement_learning\", \n",
    "        \"ensemble\", \n",
    "        \"natural_language_processing\"\n",
    "    ], \n",
    "\n",
    "    \"doc2vec\":[\n",
    "        \"natural_language_processing\"\n",
    "    ], \n",
    "\n",
    "    \"reinforcement_learning\": [\n",
    "        \"sarsa\", \n",
    "        \"markov\"\n",
    "    ],\n",
    "    \n",
    "    \"ensemble\": [\n",
    "        \"random_forest\", \n",
    "        \"adaboost\",\n",
    "        \"bagging\",\n",
    "        \"bootstrap\", \n",
    "        \"lightgbm\", \n",
    "        \"xgboost\"\n",
    "    ], \n",
    "    \n",
    "    \"data\": [\n",
    "        \"database\",\n",
    "        \"data_acquisition\",\n",
    "        \"data_mining\",\n",
    "        \"data_driven\",\n",
    "        \"data_based\"\n",
    "    ],\n",
    "    \n",
    "    \"database\": [\n",
    "        \"mapreduce\",\n",
    "        \"sql\",\n",
    "        \"hadoop\"\n",
    "    ],\n",
    "    \n",
    "    \"classification\": [\n",
    "        \"k_nearest_neighbor\",\n",
    "        \"support_vector_machine\",\n",
    "        \"neural_network\"\n",
    "    ],\n",
    "    \n",
    "    \"regression\": [ \n",
    "        \"k_nearest_neighbor\",\n",
    "        \"support_vector_machine\",\n",
    "        \"neural_network\",\n",
    "        \"ridge\",\n",
    "        \"lasso\",\n",
    "        \"autoregressive\",\n",
    "        \"support_vector_regression\"\n",
    "    ],\n",
    "    \n",
    "    \"clustering\": [\n",
    "        \"k_means\", \n",
    "        \"dbscan\"\n",
    "    ],\n",
    "    \n",
    "    \"neural_network\": [\n",
    "        \"artificial_neural_network\",\n",
    "        \"learning_vector_quantization\",\n",
    "        \"recurrent_neural_network\",\n",
    "        \"convolutional_neural_network\",\n",
    "        \"autoencoder\",\n",
    "        \"adaline\",\n",
    "        \"artificial_neural_fuzzy_inference_system\",\n",
    "        \"elman\",\n",
    "        \"attention\",\n",
    "        \"extreme_learning_machine\",\n",
    "        \"multilayer_perceptron\",\n",
    "    ],\n",
    "    \n",
    "    \"recurrent_neural_network\": [\n",
    "        \"long_short_term_memory\",\n",
    "        \"boltzmann_machine\",\n",
    "    ],\n",
    "    \n",
    "    \"convolutional_neural_network\": [\n",
    "        \"googlenet\"\n",
    "    ], \n",
    "\n",
    "    \"dimension_reduction\": [\n",
    "        \"principal_component_analysis\",\n",
    "        \"factor_analysis\",\n",
    "        \"autoencoder\"\n",
    "    ],\n",
    "    \n",
    "    \"algorithm\": [\n",
    "        \"ant_colony\", \n",
    "        \"ant_lion\", \n",
    "        \"artificial_bee_colony\", \n",
    "        \"artificial_fish_swarm\", \n",
    "        \"backtracking_search\", \n",
    "        \"bacterial_foraging\", \n",
    "        \"bat\", \n",
    "        \"bee_pollinator\", \n",
    "        \"binary_search\", \n",
    "        \"bio_inspired\", \n",
    "        \"bucket_elimination\", \n",
    "        \"crow_search\", \n",
    "        \"elite_retention\", \n",
    "        \"evolutionary\", \n",
    "        \"firefly\", \n",
    "        \"fireworks_explosion\", \n",
    "        \"flower_pollination\", \n",
    "        \"fruitfly\", \n",
    "        \"genetic_algorithm\", \n",
    "        \"golden_section\", \n",
    "        \"grasshopper\", \n",
    "        \"gravity_search\", \n",
    "        \"grey_wolf\", \n",
    "        \"imperialist_competition\", \n",
    "        \"jaya\", \n",
    "        \"leapfrog\", \n",
    "        \"honey_bee_mating\", \n",
    "        \"interior_search\", \n",
    "        \"invasive_weed\", \n",
    "        \"elephant_herding\", \n",
    "        \"particle_swarm\", \n",
    "        \"pattern_search\", \n",
    "        \"perturb_and_observe\", \n",
    "        \"shuffled_frog_leaping\", \n",
    "        \"versatile_threshold\", \n",
    "        \"monte_carlo\", \n",
    "        \"rule_based\", \n",
    "        \"dynamic_programming\"\n",
    "    ],\n",
    "    \n",
    "    \"dynamic_programming\": [\n",
    "        \"reinforcement_learning\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"keywords_knowledge_init.json\", \"w\") as j:\n",
    "    json.dump(keywords_knowledge_init, j, ensure_ascii=False, indent=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_keys_init = list(keywords_knowledge_init.keys())\n",
    "kn_values_init = list(keywords_knowledge_init.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_values_all = list(itertools.chain.from_iterable(kn_values_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFS using stack\n",
    "\n",
    "for k in kn_values_all:\n",
    "    k_start = deepcopy(k)\n",
    "    stack = []\n",
    "    \n",
    "    for idx_v, v in enumerate(kn_keys_init):\n",
    "        if k in keywords_knowledge_init[v]:\n",
    "            stack.append(idx_v)\n",
    "            if k_start == \"xgboost\":\n",
    "                print(k_start, k, v, idx_v, stack)\n",
    "            \n",
    "            k = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_, v: ensemble, xgboost\n",
      "new v_: ensemble\n"
     ]
    }
   ],
   "source": [
    "kn_values = []\n",
    "\n",
    "for v in kn_values_all:\n",
    "    v_ = deepcopy(v)\n",
    "    value_final = []\n",
    "    for k in kn_keys_init:\n",
    "        k_ = deepcopy(k)\n",
    "        for i in range(10):\n",
    "            try:\n",
    "                if v_ in keywords_knowledge_init[k_]:\n",
    "                    if v_ == \"xgboost\":\n",
    "                        print(f\"k_, v: {k_}, {v}\")\n",
    "                    value_final.append(k_)\n",
    "                    v_ = k_\n",
    "                    if v == \"xgboost\":\n",
    "                        print(f\"new v_: {v_}\")\n",
    "            except KeyError:\n",
    "                kn_values.append(value_final)\n",
    "                if v == \"xgboost\":\n",
    "                    print(f\"# v, value_final: {v}, {value_final}\\n\")\n",
    "                break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markov\n",
      "reinforcement_learning\n",
      "dynamic_programming\n",
      "algorithm\n",
      "method\n"
     ]
    }
   ],
   "source": [
    "kn_values = []\n",
    "\n",
    "for key in kn_keys:\n",
    "    value = []\n",
    "    key_ = deepcopy(key)\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            if key == \"markov\":\n",
    "                print(key_)\n",
    "            key_ = keywords_knowledge_init[key_]\n",
    "            value.append(key_)\n",
    "        except KeyError:\n",
    "            kn_values.append(value)\n",
    "#             print(key, value)\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
